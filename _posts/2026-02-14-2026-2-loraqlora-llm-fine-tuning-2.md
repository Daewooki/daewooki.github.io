---
title: "2026ë…„ 2ì›” ê¸°ì¤€: LoRA/QLoRAë¡œ LLM Fine-tuningì„ â€œí˜„ì‹¤ì ìœ¼ë¡œâ€ ëë‚´ëŠ” ë°©ë²• (ì›ë¦¬+ì‹¤ì „)"
date: 2026-02-14 02:42:51 +0900
categories: [AI, LLM]
tags: [ai, llm, trend, 2026-02]
---

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7990TVG7C7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7990TVG7C7');
</script>

## ë“¤ì–´ê°€ë©°
LLM fine-tuningì€ â€œì„±ëŠ¥ì€ ì¢‹ì€ë° ë¹„ìš©ì´ ë„ˆë¬´ ë¹„ì‹¸ë‹¤â€ê°€ ëŠ˜ ë¬¸ì œì˜€ìŠµë‹ˆë‹¤. Full fine-tuningì€ GPU ë©”ëª¨ë¦¬/ì‹œê°„/ë¹„ìš©ì´ ê¸°í•˜ê¸‰ìˆ˜ë¡œ ì»¤ì§€ê³ , ì‹¤ë¬´ì—ì„œëŠ” ë°ì´í„°ë„ ì¶©ë¶„íˆ í¬ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ 2026ë…„ 2ì›”ì—ë„ ì—¬ì „íˆ í‘œì¤€ í•´ë²•ì€ **PEFT(Parameter-Efficient Fine-Tuning)**, ê·¸ì¤‘ì—ì„œë„ **LoRA/QLoRA**ì…ë‹ˆë‹¤.

- **LoRA**: base model weightëŠ” ê³ ì •(freeze)í•˜ê³ , ì¼ë¶€ Linear layerì— **ì €ë­í¬(rank) adapter**ë§Œ í•™ìŠµí•´ íŒŒë¼ë¯¸í„°/ë©”ëª¨ë¦¬ë¥¼ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤.
- **QLoRA**: base modelì„ **4-bit quantization**ìœ¼ë¡œ ë¡œë“œí•´ VRAMì„ ë” ì•„ë¼ê³ , ê·¸ ìœ„ì— LoRA adapterë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. QLoRAëŠ” NF4, double quantization, paged optimizer ë“±ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í–ˆìŠµë‹ˆë‹¤. ([arxiv.org](https://arxiv.org/abs/2305.14314?utm_source=openai))

ê²°ë¡ ì ìœ¼ë¡œ, â€œë‚´ GPU í•œ ì¥(ì‹¬ì§€ì–´ 8~16GB)ìœ¼ë¡œë„â€ ìµœì‹  ì˜¤í”ˆì›¨ì´íŠ¸ LLMì„ ì—…ë¬´ìš©ìœ¼ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ëŠ” ê°€ì¥ í˜„ì‹¤ì ì¸ ë£¨íŠ¸ê°€ LoRA/QLoRAì…ë‹ˆë‹¤.

---

## ğŸ”§ í•µì‹¬ ê°œë…
### 1) LoRAì˜ í•µì‹¬ ì•„ì´ë””ì–´: Î”Wë¥¼ ì €ë­í¬ë¡œ ê·¼ì‚¬
Transformerì˜ Linear weightë¥¼ \(W\)ë¼ í•˜ë©´, LoRAëŠ” í•™ìŠµ ì‹œ \(W\)ë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šê³  ë‹¤ìŒì²˜ëŸ¼ ì—…ë°ì´íŠ¸ë¥¼ â€œìš°íšŒâ€í•©ë‹ˆë‹¤.

- ì›ë˜: \(W \leftarrow W + \Delta W\)
- LoRA: \(\Delta W \approx \frac{\alpha}{r} AB\)  (A: inâ†’r, B: râ†’out)  
ì¦‰, í° í–‰ë ¬ ì—…ë°ì´íŠ¸ ëŒ€ì‹  **ì‘ì€ ë‘ í–‰ë ¬(A, B)**ë§Œ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë•Œ **rank r**ê°€ ì‘ì„ìˆ˜ë¡ í•™ìŠµ íŒŒë¼ë¯¸í„°ê°€ ì¤„ê³ , í‘œí˜„ë ¥ë„ í•¨ê»˜ ì¤„ì–´ë“­ë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œëŠ” \( \alpha \)ì™€ \( r \)ì˜ ë¹„ìœ¨(ìŠ¤ì¼€ì¼ë§)ì´ ì¤‘ìš”í•˜ê³ , ê²½í—˜ì ìœ¼ë¡œ `lora_alpha = r` ë˜ëŠ” `2*r` ê°™ì€ ê·œì¹™ì´ ë„ë¦¬ ì“°ì…ë‹ˆë‹¤. ([unsloth.ai](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide?utm_source=openai))

### 2) QLoRA: 4-bitë¡œ base modelì„ â€œê³ ì •í•œ ì±„â€ ì—­ì „íŒŒëŠ” adapterë¡œ
QLoRAëŠ” base modelì„ **4-bitë¡œ quantize**í•´ VRAMì„ ì¤„ì´ë˜, í•™ìŠµ ë¶ˆì•ˆì •ì„±ì„ í”¼í•˜ê¸° ìœ„í•´ **í•™ìŠµì€ LoRA adapterì—ë§Œ** ì¼ì–´ë‚©ë‹ˆë‹¤. ì´ë•Œ ìì£¼ ì“°ëŠ” ì„¤ì •ì´:
- `bnb_4bit_quant_type="nf4"`: ì •ê·œë¶„í¬ ê°€ì •ì˜ weightì— ìµœì í™”ëœ 4-bit íƒ€ì… ([arxiv.org](https://arxiv.org/abs/2305.14314?utm_source=openai))
- `bnb_4bit_use_double_quant=True`: â€œì–‘ìí™” ìƒìˆ˜â€ë„ ë‹¤ì‹œ ì–‘ìí™”í•´ ë©”ëª¨ë¦¬ ì ˆê° ([arxiv.org](https://arxiv.org/abs/2305.14314?utm_source=openai))
- `prepare_model_for_kbit_training(model)`: k-bit í•™ìŠµì„ ìœ„í•œ ì „ì²˜ë¦¬(ì˜ˆ: layernorm ì²˜ë¦¬ ë“±) ([huggingface.co](https://huggingface.co/docs/peft/en/developer_guides/quantization?utm_source=openai))

### 3) ì–´ë””ì— LoRAë¥¼ ê½‚ì„ê¹Œ: `target_modules`
ì•„í‚¤í…ì²˜ë§ˆë‹¤ layerëª…ì´ ë‹¬ë¼ ê³¨ì¹˜ ì•„í”ˆë°, 2026ë…„ ì‹¤ë¬´ íŒì€ **ê°€ëŠ¥í•˜ë©´ â€œall-linearâ€**ì…ë‹ˆë‹¤. PEFTëŠ” `target_modules="all-linear"`ë¡œ Transformer ë‚´ë¶€ì˜ Linear/Conv1Dì— ê´‘ë²”ìœ„í•˜ê²Œ ì ìš©í•˜ëŠ” ì˜µì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/peft/en/developer_guides/quantization?utm_source=openai))

ë˜ í•˜ë‚˜ì˜ í•¨ì •: **chat templateì— íŠ¹ìˆ˜ í† í°ì´ ë“¤ì–´ê°€ë©´ embedding / lm_head ì²˜ë¦¬**ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. TRLì˜ SFTTrainer ë¬¸ì„œì—ì„œë„ `<|im_start|>`, `<|eot_id|>` ê°™ì€ special tokenì´ ìˆëŠ” ê²½ìš° `modules_to_save`ì— `embed_tokens`, `lm_head`ë¥¼ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©´ ì¶œë ¥ì´ ë§ê°€ì§ˆ ìˆ˜ ìˆë‹¤ê³  ëª…ì‹œí•©ë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/trl/v0.16.1/sft_trainer?utm_source=openai))

---

## ğŸ’» ì‹¤ì „ ì½”ë“œ
ì•„ë˜ ì½”ë“œëŠ” â€œ2026ë…„ 2ì›” ê¸°ì¤€ ê°€ì¥ í”í•œ ìŠ¤íƒâ€ì¸ **Transformers + BitsAndBytes(4-bit) + PEFT(LoRA) + TRL(SFTTrainer)** ì¡°í•©ìœ¼ë¡œ, ë¡œì»¬ GPU 1ì¥ ê¸°ì¤€ QLoRA SFTë¥¼ ì¬í˜„í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```python
# ì‹¤í–‰ ì „ ì„¤ì¹˜ ì˜ˆì‹œ:
# pip install -U "transformers>=4.45" "accelerate>=0.34" "datasets>=2.20" "trl>=0.16" "peft>=0.12" bitsandbytes

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from trl import SFTTrainer, SFTConfig

# 1) Base model ì„ íƒ: ê°€ëŠ¥í•˜ë©´ Instruct ê³„ì—´ ì¶”ì²œ(ëŒ€í™” í…œí”Œë¦¿/í† í¬ë‚˜ì´ì € ì •í•©ì„±ì´ ì¢‹ìŒ)
model_id = "Qwen/Qwen2.5-0.5B"  # ë°ëª¨ìš©(ì‘ì•„ì„œ ëˆ„êµ¬ë‚˜ ì¬í˜„ ì‰¬ì›€)
# ì‹¤ë¬´ì—ì„œëŠ” 7B~14B Instructë¥¼ QLoRAë¡œ ë§ì´ ê°

# 2) QLoRAìš© 4-bit quantization ì„¤ì • (NF4 + double quant)
#    - NF4/double quantëŠ” QLoRA ë…¼ë¬¸/PEFT ê°€ì´ë“œì—ì„œ í•µì‹¬ ì˜µì…˜ìœ¼ë¡œ ì–¸ê¸‰ë¨
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,  # Ampere+ì—ì„œ bf16 ê¶Œì¥
)

tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
# ì¼ë¶€ ëª¨ë¸ì€ pad_token ë¯¸ì •ì˜. í•™ìŠµ/ë°°ì¹˜ì—ì„œ í•„ìš”í•  ìˆ˜ ìˆìŒ.
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 3) 4-bitë¡œ base model ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=bnb_config,
)

# 4) k-bit í•™ìŠµ ì „ì²˜ë¦¬: quantized model ìœ„ì—ì„œ adapter í•™ìŠµì„ ì•ˆì •í™”
model = prepare_model_for_kbit_training(model)

# 5) LoRA ì„¤ì •
#    - QLoRA ìŠ¤íƒ€ì¼ë¡œ ìµœëŒ€í•œ ë„“ê²Œ: target_modules="all-linear"
#    - chat templateì— special tokenì´ ìˆëŠ” ëª¨ë¸ì´ë©´ modules_to_save ê³ ë ¤(íŠ¹íˆ embed/lm_head)
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules="all-linear",
    modules_to_save=["lm_head", "embed_tokens"],  # í•„ìš” ì‹œ(ëª¨ë¸/í…œí”Œë¦¿ì— ë”°ë¼ ì¡°ì •)
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, peft_config)

# 6) ë°ì´í„°ì…‹: ì˜ˆì‹œëŠ” TRLì—ì„œ ìì£¼ ì“°ëŠ” ê³µê°œ instruct ë°ì´í„°
#    - ì‹¤ë¬´ì—ì„œëŠ” "ë‚´ ì—…ë¬´ í¬ë§·"ìœ¼ë¡œ ì •ì œëœ ê³ í’ˆì§ˆ ì†ŒëŸ‰ ë°ì´í„°ê°€ ë” ì¤‘ìš”
dataset = load_dataset("trl-lib/Capybara", split="train")

# 7) SFTTrainer ì„¤ì •
#    - max_length/truncationì€ ì„±ëŠ¥ê³¼ ë¹„ìš©ì„ ì¢Œìš°
#    - ì‘ì€ GPUë©´ batch_sizeë¥¼ ë‚®ì¶”ê³  gradient_accumulationìœ¼ë¡œ ë³´ì •
sft_args = SFTConfig(
    output_dir="./qlora_sft_out",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    max_steps=200,
    logging_steps=10,
    save_steps=100,
    bf16=True,  # ê°€ëŠ¥í•˜ë©´ bf16 ê¶Œì¥
    max_length=2048,  # ë¬¸ì„œì—ì„œë„ truncation ê¸°ë³¸ ë™ì‘ ì£¼ì˜ ê°•ì¡°
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=sft_args,
)

trainer.train()

# 8) adapter ì €ì¥ (base modelì€ ê·¸ëŒ€ë¡œ, LoRAë§Œ ì €ì¥)
trainer.model.save_pretrained("./adapter_lora")

# (ì„ íƒ) ì¶”ë¡  ì‹œ:
# base model + adapterë¥¼ ë¡œë“œí•´ì„œ ì‚¬ìš©í•˜ê±°ë‚˜, merge í›„ ë‹¨ì¼ ëª¨ë¸ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŒ.
```

ìœ„ ì½”ë“œ íë¦„ì—ì„œ â€œQLoRAì˜ ë³¸ì§ˆâ€ì€ ë”± ë‘ ì¤„ì…ë‹ˆë‹¤.
- 4-bitë¡œ ë¡œë“œ: `quantization_config=bnb_config` ([huggingface.co](https://huggingface.co/docs/peft/en/developer_guides/quantization?utm_source=openai))  
- adapterë§Œ í•™ìŠµ: `get_peft_model(...)`ë¡œ LoRAë¥¼ ì–¹ê³ , baseëŠ” freeze

---

## âš¡ ì‹¤ì „ íŒ
1) **`modules_to_save`ëŠ” ìƒê°ë³´ë‹¤ ì¤‘ìš”**
ChatML/Llama ê³„ì—´ì²˜ëŸ¼ special tokenì„ ì“°ëŠ” í…œí”Œë¦¿ì—ì„œ, embedding/lm_headê°€ í•™ìŠµ/ì €ì¥ ê²½ë¡œì—ì„œ ë¹ ì§€ë©´ â€œë§ì´ ë¶•ê´´â€í•˜ê±°ë‚˜ ë¬´í•œ ë°˜ë³µ ê°™ì€ ë¬¸ì œê°€ ë‚˜ê¸°ë„ í•©ë‹ˆë‹¤. TRL ë¬¸ì„œê°€ ì´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê²½ê³ í•©ë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/trl/v0.16.1/sft_trainer?utm_source=openai))  
- í•´ê²°: `modules_to_save=["lm_head","embed_tokens"]`ë¥¼ ìš°ì„  ë„£ê³ , ëª¨ë¸ë³„ë¡œ ê²€ì¦ í›„ ìµœì†Œí™”í•˜ì„¸ìš”.

2) **QLoRA target_modulesëŠ” â€œë„“ê²Œâ€ê°€ ê¸°ë³¸ê°’**
PEFT ê°€ì´ë“œëŠ” QLoRA-styleë¡œ `target_modules="all-linear"`ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤(ë ˆì´ì–´ ëª…ì´ ë‹¤ì–‘í•œ ëª¨ë¸ì—ì„œ íŠ¹íˆ). ([huggingface.co](https://huggingface.co/docs/peft/en/developer_guides/quantization?utm_source=openai))  
ì •ë°€ íŠœë‹ì´ í•„ìš”í•˜ë©´ q/k/v/o + MLP(gate/up/down)ë¡œ ì¢í˜€ê°€ë©° ë¹„ìš©-ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì¡ìŠµë‹ˆë‹¤.

3) **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´(max_length)ëŠ” ê³§ ë¹„ìš©**
SFTTrainerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ truncationì„ ìˆ˜í–‰í•˜ê³ , tokenizer ì„¤ì •ì— ë”°ë¼ ì˜ˆìƒë³´ë‹¤ ì§§ê²Œ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•™ìŠµ ì „ì— ë°˜ë“œì‹œ `max_length`ê°€ ì˜ë„ëŒ€ë¡œì¸ì§€ í™•ì¸í•˜ì„¸ìš”. ([huggingface.co](https://huggingface.co/docs/trl/v0.16.1/sft_trainer?utm_source=openai))  
ì‹¤ë¬´ì ìœ¼ë¡œëŠ” â€œìµœëŒ€ ê¸¸ì´â€ë³´ë‹¤ â€œë°ì´í„°ì˜ ê¸¸ì´ ë¶„í¬â€ë¥¼ ë¨¼ì € ë³´ê³ , íŒ¨ë”©/ì˜ë¦¼ì„ ì¤„ì´ë„ë¡ ìƒ˜í”Œì„ ì¬êµ¬ì„±í•˜ëŠ” í¸ì´ íš¨ê³¼ê°€ í½ë‹ˆë‹¤.

4) **â€œí›ˆë ¨ì€ completionsë§Œâ€ì´ ì˜ì™¸ë¡œ ì˜ ë¨¹íŒë‹¤**
ì§€ì‹œë¬¸ê¹Œì§€ ëª¨ë‘ lossì— ë„£ìœ¼ë©´, ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ë¥¼ â€œì •ë‹µì²˜ëŸ¼â€ ì™¸ìš°ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. completions-only(assistant êµ¬ê°„ë§Œ loss)ë¡œ ê°€ë©´ ì§€ì‹œ ë”°ë¥´ê¸° í’ˆì§ˆì´ ì¢‹ì•„ì§€ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤(íŠ¹íˆ multi-turn). ì´ ì „ëµì€ QLoRA ê³„ì—´ ë ˆì‹œí”¼ì—ì„œ ìì£¼ ì–¸ê¸‰ë©ë‹ˆë‹¤. ([unsloth.ai](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide?utm_source=openai))

5) **ê²€ì¦ì€ â€œì •ëŸ‰+ì •ì„±â€ ë‘˜ ë‹¤**
QLoRA ë…¼ë¬¸ë„ ë²¤ì¹˜ë§ˆí¬ì˜ ì‹ ë¢°ì„± ë¬¸ì œë¥¼ ì§€ì í•˜ê³ , í‰ê°€ê°€ ìƒê°ë³´ë‹¤ ì–´ë µë‹¤ëŠ” ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ([arxiv.org](https://arxiv.org/abs/2305.14314?utm_source=openai))  
ì‹¤ë¬´ì—ì„œëŠ”:
- ê³ ì •ëœ 50~200ê°œ â€œì—…ë¬´ ëŒ€í‘œ ì§ˆë¬¸ ì„¸íŠ¸â€ë¥¼ ë§Œë“¤ê³ 
- í•™ìŠµ ì „/í›„ë¥¼ ë™ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ë¹„êµ
- ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë¥¼ ë°ì´í„°ì— ë˜ë¨¹ì„(ë°ì´í„° íë ˆì´ì…˜)ì´ ê°€ì¥ ë¹ ë¥¸ ê°œì„  ë£¨í”„ì…ë‹ˆë‹¤.

---

## ğŸš€ ë§ˆë¬´ë¦¬
LoRAëŠ” â€œì ì€ íŒŒë¼ë¯¸í„°ë¡œ ì›í•˜ëŠ” ì„±ê²©ë§Œ ë§ì…íˆëŠ”â€ ë°©ë²•ì´ê³ , QLoRAëŠ” ê±°ê¸°ì— **4-bit quantization**ì„ ë”í•´ â€œë‚´ GPU í•œ ì¥ì—ì„œë„â€ fine-tuningì„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“  ë°©ì‹ì…ë‹ˆë‹¤. 2026ë…„ 2ì›” ê¸°ì¤€ ì‹¤ë¬´ ë² ì´ìŠ¤ë¼ì¸ì€:

- QLoRA: `BitsAndBytesConfig(load_in_4bit=True, nf4, double_quant)` + `prepare_model_for_kbit_training` ([huggingface.co](https://huggingface.co/docs/peft/en/developer_guides/quantization?utm_source=openai))  
- LoRA: `target_modules="all-linear"`ë¡œ ì‹œì‘, í•„ìš”í•˜ë©´ ì¢í˜€ê°€ê¸° ([huggingface.co](https://huggingface.co/docs/peft/en/developer_guides/quantization?utm_source=openai))  
- TRL SFTTrainer: truncation/max_lengthì™€ `modules_to_save` í•¨ì •ì„ ë¨¼ì € ì¡ê¸° ([huggingface.co](https://huggingface.co/docs/trl/v0.16.1/sft_trainer?utm_source=openai))  

ë‹¤ìŒ í•™ìŠµ ì¶”ì²œì€ ë‘ ê°ˆë˜ì…ë‹ˆë‹¤.
1) **ë°ì´í„° ë ˆì‹œí”¼ ê³ ë„í™”**: completions-only, ë©€í‹°í„´ êµ¬ì„±, ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¤‘ì‹¬ ì¦ê°•  
2) **í›„ì† ì •ë ¬(Alignment)**: SFT ì´í›„ DPO/ORPO ê°™ì€ ì„ í˜¸ ìµœì í™”ë¡œ â€œë§íˆ¬/ì •ì±…/ì•ˆì „â€ì„ ë” ì •êµí•˜ê²Œ ë§Œë“¤ê¸°

ì›í•˜ë©´, (1) íŠ¹ì • ëª¨ë¸(Llama ê³„ì—´/Qwen ê³„ì—´/Gemma ê³„ì—´) ì¤‘ ì–´ë–¤ ê±¸ ëª©í‘œë¡œ í•˜ëŠ”ì§€, (2) GPU VRAM, (3) ë°ì´í„° í¬ë§·(ì˜ˆ: ShareGPT/ChatML/ìì²´ JSON)ì„ ì•Œë ¤ì£¼ë©´ ìœ„ ì½”ë“œë¥¼ ê·¸ í™˜ê²½ì— ë§ì¶° â€œë°”ë¡œ ëŒë¦´ ìˆ˜ ìˆëŠ” í˜•íƒœâ€ë¡œ ë” ì¢í˜€ì„œ êµ¬ì„±í•´ë“œë¦´ê²Œìš”.
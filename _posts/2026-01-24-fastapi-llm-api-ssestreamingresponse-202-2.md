---
title: "FastAPIë¡œ LLM API ì„œë²„ â€œì§„ì§œ ìŠ¤íŠ¸ë¦¬ë°â€ ë§Œë“¤ê¸°: SSE/StreamingResponse í•¨ì •ê¹Œì§€ (2026ë…„ 1ì›” ê¸°ì¤€)"
date: 2026-01-24 02:12:14 +0900
categories: [Backend, API]
tags: [backend, api, trend, 2026-01]
---

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7990TVG7C7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7990TVG7C7');
</script>

## ë“¤ì–´ê°€ë©°
LLM API ì„œë²„ì—ì„œ â€œìŠ¤íŠ¸ë¦¬ë°â€ì€ ë‹¨ìˆœ UX ì˜µì…˜ì´ ì•„ë‹ˆë¼ **ì„œë²„ ë¹„ìš©/ì§€ì—°/íƒ€ì„ì•„ì›ƒ**ì„ ì¢Œìš°í•˜ëŠ” í•µì‹¬ ì„¤ê³„ì…ë‹ˆë‹¤. ê¸´ ì‘ë‹µì„ í•œ ë²ˆì— ë°˜í™˜í•˜ë©´ TTFB(Time To First Byte)ê°€ ì»¤ì§€ê³ , ì‚¬ìš©ìëŠ” â€œë©ˆì¶˜ ê²ƒì²˜ëŸ¼â€ ëŠë¼ë©°, ì„œë²„ëŠ” ì „ì²´ ê²°ê³¼ë¥¼ ë²„í¼ë§í•˜ëŠ” ë™ì•ˆ ë©”ëª¨ë¦¬Â·ì»¤ë„¥ì…˜ì„ ì˜¤ë˜ ë¬¼ê³  ìˆê²Œ ë©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ í† í° ë‹¨ìœ„ë¡œ í˜ë ¤ë³´ë‚´ë©´ **ì²« í† í°ê¹Œì§€ì˜ ì§€ì—°ì„ ìµœì†Œí™”**í•˜ê³ , ì¤‘ê°„ì— ì‚¬ìš©ìê°€ ì·¨ì†Œí•˜ë©´ **ì¦‰ì‹œ ì—°ì‚°ì„ ëŠì–´ ë¹„ìš©ì„ ì ˆê°**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2026ë…„ 1ì›” ì‹œì ì—ì„œ FastAPIë¡œ ìŠ¤íŠ¸ë¦¬ë°ì„ êµ¬í˜„í•  ë•ŒëŠ” í¬ê²Œ ë‘ ê°ˆë˜ê°€ ì‹¤ë¬´ì—ì„œ ë§ì´ ì“°ì…ë‹ˆë‹¤.

- **SSE(Server-Sent Events)**: ë¸Œë¼ìš°ì € ì¹œí™”ì (ìë™ ì¬ì—°ê²°), â€œì„œë²„ â†’ í´ë¼ì´ì–¸íŠ¸â€ ë‹¨ë°©í–¥ í† í° ìŠ¤íŠ¸ë¦¬ë°ì— ìµœì . FastAPI ìƒíƒœê³„ì—ì„œëŠ” `sse-starlette`ì˜ `EventSourceResponse`ê°€ ì‚¬ì‹¤ìƒ í‘œì¤€ ë„êµ¬ì²˜ëŸ¼ ì“°ì…ë‹ˆë‹¤. ([pypi.org](https://pypi.org/project/sse-starlette/?utm_source=openai))  
- **StreamingResponse(ë°”ì´íŠ¸/í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼)**: í”„ë¡œí† ì½œ ì œì•½ì´ ì ê³  ë²”ìš©ì´ì§€ë§Œ, í”„ë¡ì‹œ ë²„í¼ë§/í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„ì— ë”°ë¼ â€œìŠ¤íŠ¸ë¦¬ë°ì´ ì•ˆ ë˜ëŠ” ê²ƒì²˜ëŸ¼â€ ë³´ì´ëŠ” ë¬¸ì œê°€ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤(íŠ¹íˆ ë¸Œë¼ìš°ì €/axios ì¡°í•©). ([stackoverflow.com](https://stackoverflow.com/questions/75876640/fastapis-streamingresponse-doesnt-return-stream?utm_source=openai))  

ë˜í•œ, OpenAI API ìì²´ë„ ìŠ¤íŠ¸ë¦¬ë°ì„ SSE í˜•íƒœë¡œ ì œê³µí•˜ë¯€ë¡œ(ì„œë²„ê°€ OpenAI SSEë¥¼ ë°›ì•„ì„œ ë‹¤ì‹œ í´ë¼ì´ì–¸íŠ¸ì— SSEë¡œ ì¤‘ê³„í•˜ëŠ” êµ¬ì¡°) íŒŒì´í”„ë¼ì¸ì„ ê¹”ë”í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ([platform.openai.com](https://platform.openai.com/docs/guides/streaming-responses?utm_source=openai))  

---

## ğŸ”§ í•µì‹¬ ê°œë…
### 1) â€œìŠ¤íŠ¸ë¦¬ë°â€ì˜ ë³¸ì§ˆ: flush ê°€ëŠ¥í•œ ì‘ì€ chunkì˜ ì—°ì†
HTTPì—ì„œ ìŠ¤íŠ¸ë¦¬ë°ì€ ì‘ë‹µì„ ëê¹Œì§€ ë§Œë“  ë’¤ ë³´ë‚´ëŠ” ê²Œ ì•„ë‹ˆë¼, **ë§Œë“¤ì–´ì§€ëŠ” ëŒ€ë¡œ ì‘ì€ ì¡°ê°(chunk)ì„ ë°”ë¡œ ì „ì†¡**í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. í”„ë ˆì„ì›Œí¬ ê´€ì ì—ì„œëŠ” â€œasync generatorê°€ `yield`í•˜ëŠ” ë°”ì´íŠ¸/ë¬¸ìì—´ì„ ì¦‰ì‹œ ë„¤íŠ¸ì›Œí¬ë¡œ í˜ë ¤ë³´ë‚´ëŠ”ê°€â€ê°€ í•µì‹¬ì…ë‹ˆë‹¤.

ì—¬ê¸°ì„œ í”í•œ ì˜¤í•´ê°€ í•˜ë‚˜ ìˆìŠµë‹ˆë‹¤. ì„œë²„ ì½”ë“œê°€ `yield`ë¥¼ í•˜ê³  ìˆì–´ë„,
- ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ(Nginx/Cloudflare ë“±)ê°€ ë²„í¼ë§í•˜ê±°ë‚˜
- í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìŠ¤íŠ¸ë¦¼ ì†Œë¹„ë¥¼ ì œëŒ€ë¡œ ëª» í•˜ê±°ë‚˜
- gzip/ì••ì¶•/ë¯¸ë“¤ì›¨ì–´ê°€ ë°”ë””ë¥¼ ë‹¤ì‹œ ëª¨ì•„ë²„ë¦¬ë©´  
**ì‚¬ìš©ìëŠ” â€˜í•œ ë²ˆì— ë„ì°©â€™í•˜ëŠ” ê²ƒì²˜ëŸ¼** ë³´ê²Œ ë©ë‹ˆë‹¤.

`sse-starlette` ë¬¸ì„œ/ì„¤ëª…ì—ì„œë„ Nginx ë²„í¼ë§ì´ SSEë¥¼ ë§ê°€ëœ¨ë¦¬ëŠ” ëŒ€í‘œ ì‚¬ë¡€ë¡œ ì–¸ê¸‰ë˜ë©°, `X-Accel-Buffering: no` ë˜ëŠ” `proxy_buffering off` ê°™ì€ ëŒ€ì‘ì´ ì¤‘ìš”í•˜ë‹¤ê³  ë§í•©ë‹ˆë‹¤. ([pypi.org](https://pypi.org/project/sse-starlette/?utm_source=openai))  

### 2) SSE vs WebSocket: LLM í† í°ì—ëŠ” SSEê°€ ë” â€œì •ë‹µâ€ì¸ ê²½ìš°ê°€ ë§ë‹¤
LLM ì±„íŒ… UIì˜ ëŒ€ë¶€ë¶„ì€ â€œì„œë²„ê°€ í† í°ì„ í˜ë ¤ì£¼ê³  í´ë¼ì´ì–¸íŠ¸ëŠ” í‘œì‹œâ€í•˜ëŠ” ë‹¨ë°©í–¥ì…ë‹ˆë‹¤. ì´ë•Œ SSEëŠ”:
- HTTP ê¸°ë°˜(ì—…ê·¸ë ˆì´ë“œ ë¶ˆí•„ìš”)
- ë¸Œë¼ìš°ì € `EventSource`ê°€ ê¸°ë³¸ ì œê³µ(ìë™ ì¬ì—°ê²°)
- ì„œë²„ êµ¬í˜„ì´ ë‹¨ìˆœ(í•œ ì»¤ë„¥ì…˜ì—ì„œ ì´ë²¤íŠ¸ë¥¼ ê³„ì† push)

íŠ¹íˆ `sse-starlette`ëŠ” **ping(heartbeat), disconnect ê°ì§€, send timeout** ê°™ì€ ìš´ì˜ ìš”ì†Œë¥¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë ˆë²¨ì—ì„œ ë‹¤ë£¨ëŠ” ë°©í–¥ìœ¼ë¡œ ë°œì „í–ˆìŠµë‹ˆë‹¤. ([github.com](https://github.com/sysid/sse-starlette?utm_source=openai))  

### 3) disconnect/cancelì€ â€œì˜µì…˜â€ì´ ì•„ë‹ˆë¼ ë¹„ìš© í†µì œ ì¥ì¹˜
ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê±´ â€œí´ë¼ì´ì–¸íŠ¸ê°€ íƒ­ì„ ë‹«ì•˜ëŠ”ë°ë„ ì„œë²„ê°€ ê³„ì† LLMì„ ëŒë¦¬ëŠ”â€ ìƒí™©ì„ ë§‰ëŠ” ê²ë‹ˆë‹¤. FastAPI/Starletteì—ì„œëŠ” `request.is_disconnected()`ë¡œ ì—°ê²° ì¢…ë£Œë¥¼ ê°ì§€í•´ ë£¨í”„ë¥¼ ëŠëŠ” íŒ¨í„´ì´ ë„ë¦¬ ì“°ì…ë‹ˆë‹¤. ([dev.to](https://dev.to/bobbyiliev/how-to-use-server-sent-events-sse-with-fastapi-52fo?utm_source=openai))  

ë˜ í•˜ë‚˜ ì‹¤ë¬´ íŒ: StreamingResponse/SSE ì‘ë‹µì—ì„œ ì¿ í‚¤/í—¤ë”ë¥¼ ì„¤ì •í•  ë•ŒëŠ” **ë°˜ë“œì‹œ ìµœì¢…ì ìœ¼ë¡œ ë°˜í™˜í•  Response ê°ì²´ì— ì„¤ì •**í•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, FastAPI ë¼ìš°íŠ¸ ì¸ìë¡œ ë°›ì€ `response: Response`ì— set_cookie í•´ë†“ê³  `StreamingResponse(...)`ë¥¼ ë°˜í™˜í•˜ë©´ í—¤ë”ê°€ ì•ˆ ë‚˜ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜í™˜í•  streaming response ì¸ìŠ¤í„´ìŠ¤ì— ì§ì ‘ `set_cookie`ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤. ([stackoverflow.com](https://stackoverflow.com/questions/79528427/cannot-set-cookies-when-using-streamingresponse-in-fastapi-route-for-sse-server?utm_source=openai))  

---

## ğŸ’» ì‹¤ì „ ì½”ë“œ
ì•„ë˜ ì˜ˆì œëŠ” **(1) LLM í† í° ìƒì„±(ëª¨ì˜), (2) SSEë¡œ í† í° ìŠ¤íŠ¸ë¦¬ë°, (3) í´ë¼ì´ì–¸íŠ¸ disconnect ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨, (4) í”„ë¡ì‹œ ë²„í¼ë§ ë°©ì§€ í—¤ë”**ê¹Œì§€ í¬í•¨í•œ â€œAPI ì„œë²„ ë¼ˆëŒ€â€ì…ë‹ˆë‹¤.

```python
# main.py
import asyncio
import json
from typing import AsyncGenerator

from fastapi import FastAPI, Request
from sse_starlette import EventSourceResponse, ServerSentEvent

app = FastAPI()

async def fake_llm_stream(prompt: str) -> AsyncGenerator[str, None]:
    """
    ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì—¬ê¸°ì—ì„œ OpenAI/ìì²´ LLMì˜ stream ì´ë²¤íŠ¸ë¥¼ ë°›ì•„
    delta tokenì„ yield í•˜ëŠ” êµ¬ì¡°ê°€ ë©ë‹ˆë‹¤.
    """
    for tok in ["ì´", "ê±´", " ", "ìŠ¤íŠ¸", "ë¦¬", "ë°", " ", "ë°", "ëª¨", " ", "ì…", "ë‹ˆ", "ë‹¤"]:
        await asyncio.sleep(0.05)  # í† í° ìƒì„± ì§€ì—°(ëª¨ì˜)
        yield tok

@app.get("/v1/chat/stream")
async def chat_stream(request: Request, prompt: str):
    async def event_generator() -> AsyncGenerator[dict, None]:
        """
        SSEëŠ” í…ìŠ¤íŠ¸ í”„ë¡œí† ì½œì´ë¼ "event/data/id/retry" í˜•íƒœë¡œ ë³´ëƒ…ë‹ˆë‹¤.
        dataì— JSON ë¬¸ìì—´ì„ ë„£ì–´ë‘ë©´ í”„ë¡ íŠ¸ì—ì„œ ë‹¤ë£¨ê¸° í¸í•©ë‹ˆë‹¤.
        """
        # ì²« ì´ë²¤íŠ¸: ë©”íƒ€/ì‹œì‘ ì‹ í˜¸
        yield {"event": "start", "data": json.dumps({"prompt": prompt})}

        try:
            async for tok in fake_llm_stream(prompt):
                # 1) í´ë¼ì´ì–¸íŠ¸ê°€ ëŠê²¼ìœ¼ë©´ ì¦‰ì‹œ ì¤‘ë‹¨ (ë¹„ìš© ì ˆê°)
                if await request.is_disconnected():
                    break

                # 2) í† í°ì„ SSE ì´ë²¤íŠ¸ë¡œ í‘¸ì‹œ
                yield {"event": "token", "data": json.dumps({"delta": tok})}

            # ì •ìƒ ì¢…ë£Œ ì‹ í˜¸
            yield {"event": "done", "data": json.dumps({"reason": "end"})}

        except asyncio.CancelledError:
            # ì„œë²„ ì¸¡ cancel(ì˜ˆ: ì›Œì»¤ ì¢…ë£Œ, íƒ€ì„ì•„ì›ƒ ë“±)ë„ ëª…í™•íˆ ì²˜ë¦¬
            # í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ í›„ ì˜ˆì™¸ ì¬ë°œìƒ
            raise

    # Nginx ë“±ì—ì„œ SSE ë²„í¼ë§ì„ ë„ëŠ” í—¤ë”(ê°€ëŠ¥í•˜ë©´ ê°•ì œ)
    # sse-starletteì—ì„œë„ Nginx ë²„í¼ë§ ì´ìŠˆì™€ X-Accel-Buffering: noë¥¼ ì–¸ê¸‰í•©ë‹ˆë‹¤.
    headers = {
        "Cache-Control": "no-cache",
        "X-Accel-Buffering": "no",
    }

    # ping: ì¤‘ê°„ì— ì•„ë¬´ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ LBê°€ ëŠëŠ” ê²½ìš°ê°€ ìˆì–´ heartbeatê°€ ì¤‘ìš”
    # send_timeout: í´ë¼ì´ì–¸íŠ¸ê°€ ì½ì§€ ì•Šì•„ sendê°€ ë©ˆì¶”ëŠ” "hanging" ë°©ì§€ ì˜µì…˜(ë²„ì „ì— ë”°ë¼ ì§€ì›)
    return EventSourceResponse(
        event_generator(),
        headers=headers,
        ping=15,
        ping_message_factory=lambda: ServerSentEvent(comment="keep-alive"),
    )
```

ì‹¤í–‰:
```bash
pip install fastapi uvicorn sse-starlette
uvicorn main:app --host 0.0.0.0 --port 8000
```

ë¸Œë¼ìš°ì € ì¸¡ì€ `EventSource("/v1/chat/stream?prompt=...")`ë¡œ ë°”ë¡œ ë¶™ì´ë©´ ë©ë‹ˆë‹¤(SSEëŠ” ë¸Œë¼ìš°ì € ê¸°ë³¸ ì§€ì›).

---

## âš¡ ì‹¤ì „ íŒ
1) **SSEëŠ” í”„ë¡ì‹œ/CDN ë²„í¼ë§ì´ â€˜ì§„ì§œ ì â€™**  
ë¡œì»¬ì—ì„œëŠ” ì˜ ìŠ¤íŠ¸ë¦¬ë°ë˜ëŠ”ë° ìš´ì˜ì—ì„œ â€œí•œ ë²ˆì— ë„ì°©â€í•˜ë©´, ëŒ€ê°œ ì•± ë¬¸ì œê°€ ì•„ë‹ˆë¼ **ì¤‘ê°„ ê³„ì¸µ ë²„í¼ë§**ì…ë‹ˆë‹¤. `sse-starlette`ëŠ” Nginx ê¸°ë³¸ ë²„í¼ë§ê³¼ í•´ê²°ì±…(`X-Accel-Buffering: no`, `proxy_buffering off`)ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•©ë‹ˆë‹¤. ([pypi.org](https://pypi.org/project/sse-starlette/?utm_source=openai))  

2) **heartbeat(ping) ì£¼ê¸°ëŠ” LB timeoutë³´ë‹¤ ì§§ê²Œ**  
SSEëŠ” ì—°ê²°ì„ ì˜¤ë˜ ìœ ì§€í•˜ë¯€ë¡œ, LB/Ingress idle timeoutì— ê±¸ë¦¬ì§€ ì•Šê²Œ **ì£¼ê¸°ì ìœ¼ë¡œ comment ping**ì„ ë³´ë‚´ì„¸ìš”. `sse-starlette`ëŠ” pingì„ ê¸°ë³¸ ê°œë…ìœ¼ë¡œ ì œê³µí•˜ê³  ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆë„ ì§€ì›í•©ë‹ˆë‹¤. ([pypi.org](https://pypi.org/project/sse-starlette/2.1.0/?utm_source=openai))  

3) **disconnect ê°ì§€ ì—†ìœ¼ë©´ LLM ë¹„ìš©ì´ ìƒˆëŠ” êµ¬ì¡°**  
`request.is_disconnected()` ì²´í¬ëŠ” â€œì˜ˆì˜ê²Œ ì¢…ë£Œâ€ê°€ ì•„ë‹ˆë¼ **ìš”ê¸ˆ í†µì œ**ì…ë‹ˆë‹¤. íŠ¹íˆ upstream(OpenAI ë“±)ì—ë„ cancelì„ ì „íŒŒí•  ìˆ˜ ìˆê²Œ ì„¤ê³„(ìš”ì²­ task cancel, ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ)í•˜ëŠ” ê²Œ ì¤‘ìš”í•©ë‹ˆë‹¤. ([dev.to](https://dev.to/bobbyiliev/how-to-use-server-sent-events-sse-with-fastapi-52fo?utm_source=openai))  

4) **ì¿ í‚¤/í—¤ë” ì„¤ì •ì€ â€˜ë°˜í™˜í•  ìŠ¤íŠ¸ë¦¬ë° Responseâ€™ì— ì§ì ‘**  
StreamingResponse/SSEì—ì„œ `response.set_cookie()`ë¥¼ í–ˆëŠ”ë° ë°˜ì˜ì´ ì•ˆ ë˜ë©´, ë¼ìš°íŠ¸ íŒŒë¼ë¯¸í„° `Response`ê°€ ì•„ë‹ˆë¼ **ì‹¤ì œ ë°˜í™˜ ê°ì²´(StreamingResponse/EventSourceResponse)**ì— `set_cookie`ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤. ([stackoverflow.com](https://stackoverflow.com/questions/79528427/cannot-set-cookies-when-using-streamingresponse-in-fastapi-route-for-sse-server?utm_source=openai))  

5) **OpenAI upstreamë„ SSEë¼ì„œ â€œSSE â†’ SSE ì¤‘ê³„â€ê°€ ìì—°ìŠ¤ëŸ½ë‹¤**  
OpenAIëŠ” ìŠ¤íŠ¸ë¦¬ë°ì„ SSE ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œí™”í•˜ê³  ìˆì–´, ì„œë²„ê°€ upstream ì´ë²¤íŠ¸ë¥¼ ë°›ì•„ í† í° ë‹¨ìœ„ë¡œ ê·¸ëŒ€ë¡œ(ë˜ëŠ” ë³€í™˜í•´ì„œ) í˜ë ¤ë³´ë‚´ëŠ” íŒŒì´í”„ë¼ì¸ì´ ê¹”ë”í•©ë‹ˆë‹¤. ([platform.openai.com](https://platform.openai.com/docs/guides/streaming-responses?utm_source=openai))  

---

## ğŸš€ ë§ˆë¬´ë¦¬
FastAPIë¡œ LLM API ì„œë²„ë¥¼ ë§Œë“¤ ë•Œ ìŠ¤íŠ¸ë¦¬ë°ì€ â€œê¸°ëŠ¥â€ì´ ì•„ë‹ˆë¼ **ì•„í‚¤í…ì²˜ ì„ íƒ(SSE/StreamingResponse) + ìš´ì˜ ìš”ì†Œ(ë²„í¼ë§/heartbeat/disconnect/cancel)**ì˜ ì¡°í•©ì…ë‹ˆë‹¤. 2026ë…„ 1ì›” ê¸°ì¤€ìœ¼ë¡œëŠ”, ë¸Œë¼ìš°ì € í† í° ìŠ¤íŠ¸ë¦¬ë°ì´ë¼ë©´ `sse-starlette`ì˜ `EventSourceResponse`ê°€ ìš´ì˜ ì¹œí™”ì ì¸ ì„ íƒì´ê³ (í•‘/ì¢…ë£Œ/í—¤ë”), ê·¸ ìœ„ì— **disconnect ì¦‰ì‹œ ì¤‘ë‹¨**ê³¼ **í”„ë¡ì‹œ ë²„í¼ë§ ì°¨ë‹¨**ì„ ë°˜ë“œì‹œ ì–¹ëŠ” ê²Œ ì‹¤ë¬´ ì •ë‹µì— ê°€ê¹ìŠµë‹ˆë‹¤. ([github.com](https://github.com/sysid/sse-starlette?utm_source=openai))  

ë‹¤ìŒ í•™ìŠµìœ¼ë¡œëŠ”:
- OpenAI(ë˜ëŠ” ë‹¤ë¥¸ LLM) ìŠ¤íŠ¸ë¦¼ ì´ë²¤íŠ¸ë¥¼ **ì„œë²„ì—ì„œ ì–´ë–»ê²Œ cancel ì „íŒŒ**í• ì§€(ìš”ì²­ ìŠ¤ì½”í”„ cancel, íƒ€ì„ì•„ì›ƒ ì „ëµ) ([platform.openai.com](https://platform.openai.com/docs/guides/streaming-responses?utm_source=openai))  
- â€œí† í° ìœ ì‹¤/ì§€ì—°â€ ê°™ì€ ë„¤íŠ¸ì›Œí¬ í’ˆì§ˆ ë¬¸ì œê¹Œì§€ ê³ ë ¤í•œ ìŠ¤íŠ¸ë¦¬ë° ì „ì†¡ ë°©ì‹(ì—°êµ¬ ê´€ì ) ([arxiv.org](https://arxiv.org/abs/2401.12961?utm_source=openai))  

ì›í•˜ì‹œë©´ ìœ„ ì½”ë“œì— **OpenAI Responses API ìŠ¤íŠ¸ë¦¬ë°ì„ ì‹¤ì œë¡œ ë¶™ì´ëŠ” ë²„ì „(Async client + ì´ë²¤íŠ¸ ë§¤í•‘ + usage ì§‘ê³„ + ì—ëŸ¬ ì´ë²¤íŠ¸ í‘œì¤€í™”)**ê¹Œì§€ í™•ì¥í•´ì„œ ì‘ì„±í•´ë“œë¦´ê²Œìš”.
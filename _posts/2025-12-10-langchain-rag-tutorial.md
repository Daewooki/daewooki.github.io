---
title: "LangChainìœ¼ë¡œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•í•˜ê¸° ğŸ“š"
date: 2025-12-10 10:00:00 +0900
categories: [AI, LangChain]
tags: [langchain, rag, llm, ai, vector-db]
---

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7990TVG7C7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7990TVG7C7');
</script>
## RAGë€?

**RAG (Retrieval-Augmented Generation)**ëŠ” LLMì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.

LLMì˜ ë¬¸ì œì :
- âŒ í•™ìŠµ ë°ì´í„° ì´í›„ ì •ë³´ ëª¨ë¦„
- âŒ íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œ ëª¨ë¦„
- âŒ í• ë£¨ì‹œë„¤ì´ì…˜ (ê±°ì§“ ì •ë³´ ìƒì„±)

RAGì˜ í•´ê²°ì±…:
- âœ… ì™¸ë¶€ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
- âœ… ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
- âœ… ê·¼ê±° ê¸°ë°˜ ì‘ë‹µ ìƒì„±

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
[ë¬¸ì„œ] â†’ [ì²­í‚¹] â†’ [ì„ë² ë”©] â†’ [Vector DB]
                                    â†“
[ì§ˆë¬¸] â†’ [ì„ë² ë”©] â†’ [ìœ ì‚¬ë„ ê²€ìƒ‰] â†’ [ê´€ë ¨ ë¬¸ì„œ]
                                    â†“
                    [LLM + ì»¨í…ìŠ¤íŠ¸] â†’ [ì‘ë‹µ]
```

---

## ğŸ’» êµ¬í˜„ ì½”ë“œ

### 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install langchain langchain-openai chromadb pypdf
```

### 2. ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# PDF ë¬¸ì„œ ë¡œë“œ
loader = PyPDFLoader("company_docs.pdf")
documents = loader.load()

# ì²­í‚¹ (ë¬¸ì„œë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• )
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ".", " "]
)
chunks = text_splitter.split_documents(documents)

print(f"ì´ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„±")
```

### 3. ë²¡í„° DB êµ¬ì¶•

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

# ì„ë² ë”© ëª¨ë¸
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Chroma DBì— ì €ì¥
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# ê²€ìƒ‰ê¸° ìƒì„±
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰
)
```

### 4. RAG ì²´ì¸ êµ¬ì„±

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

prompt = ChatPromptTemplate.from_template(template)

# RAG ì²´ì¸
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ì‹¤í–‰
response = rag_chain.invoke("íšŒì‚¬ì˜ ì—°ì°¨ ì •ì±…ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?")
print(response)
```

---

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥

### 1. Hybrid Search (í‚¤ì›Œë“œ + ì˜ë¯¸ ê²€ìƒ‰)

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# BM25 (í‚¤ì›Œë“œ ê¸°ë°˜)
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 3

# ì•™ìƒë¸” (í•˜ì´ë¸Œë¦¬ë“œ)
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, retriever],
    weights=[0.4, 0.6]
)
```

### 2. Rerankerë¡œ ì •í™•ë„ í–¥ìƒ

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank

reranker = CohereRerank(model="rerank-multilingual-v3.0")
compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=retriever
)
```

### 3. ëŒ€í™” ê¸°ë¡ ìœ ì§€

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory
)
```

---

## ğŸ’¡ ì‹¤ì „ íŒ

1. **ì²­í¬ ì‚¬ì´ì¦ˆëŠ” ì‹¤í—˜ì ìœ¼ë¡œ** - 500~1500 ì‚¬ì´ì—ì„œ í…ŒìŠ¤íŠ¸
2. **ì˜¤ë²„ë©ì€ í•„ìˆ˜** - ë¬¸ë§¥ì´ ëŠê¸°ì§€ ì•Šë„ë¡
3. **ë©”íƒ€ë°ì´í„° í™œìš©** - ì¶œì²˜, í˜ì´ì§€ ë²ˆí˜¸ ë“±
4. **í‰ê°€ ì§€í‘œ ì„¤ì •** - Faithfulness, Relevance ì¸¡ì •

---

## ğŸ¯ ë§ˆë¬´ë¦¬

RAGëŠ” ê¸°ì—…ìš© AI ì±—ë´‡ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.

ì´ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì´í•´í•˜ë©´:
- ì‚¬ë‚´ ë¬¸ì„œ Q&A ë´‡
- ê³ ê° ì§€ì› ì±—ë´‡
- ë²•ë¥ /ì˜ë£Œ ë¬¸ì„œ ë¶„ì„

ë“± ë‹¤ì–‘í•œ ì‘ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!

ë‹¤ìŒ ê¸€ì—ì„œëŠ” **í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œ ìµœì í™”**ë¥¼ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤.


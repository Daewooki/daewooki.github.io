---
title: "2026ë…„ 2ì›” ê¸°ì¤€ vLLMÂ·TGIÂ·Ollama ë°°í¬ë²•: â€œë¡œì»¬ ê°œë°œ â†’ í”„ë¡œë•ì…˜ ì„œë¹™â€ê¹Œì§€ í•œ ë²ˆì— ì •ë¦¬"
date: 2026-02-07 02:40:41 +0900
categories: [AI, MLOps]
tags: [ai, mlops, trend, 2026-02]
---

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7990TVG7C7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7990TVG7C7');
</script>

## ë“¤ì–´ê°€ë©°
LLM ì„œë¹™ì€ ì´ì œ â€œëª¨ë¸ì„ ë„ìš´ë‹¤â€ì—ì„œ ëë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê°™ì€ GPUë¼ë„ **ë™ì‹œì„±(concurrency)**, **KV cache ë©”ëª¨ë¦¬ íš¨ìœ¨**, **batching ì „ëµ**, **ê´€ì¸¡ì„±(observability)**ì— ë”°ë¼ ë¹„ìš©ê³¼ UX(íŠ¹íˆ TTFT/P99)ê°€ í¬ê²Œ ê°ˆë¦½ë‹ˆë‹¤. ê·¸ë˜ì„œ íŒ€ë“¤ì´ ë³´í†µ ë‹¤ìŒ 3ì¶•ìœ¼ë¡œ ë„êµ¬ë¥¼ ë‚˜ëˆ  ì”ë‹ˆë‹¤.

- **Ollama**: ë¡œì»¬/ê°œë°œ í™˜ê²½ì—ì„œ ê°€ì¥ ë¹ ë¥´ê²Œ â€œëŒì•„ê°€ëŠ” ìƒíƒœâ€ ë§Œë“¤ê¸°(ë‹¨ìˆœí•¨ ìš°ì„ )
- **vLLM**: ë†’ì€ ë™ì‹œì„±ê³¼ ì²˜ë¦¬ëŸ‰(throughput)ì„ ë½‘ì•„ì•¼ í•˜ëŠ” í”„ë¡œë•ì…˜(ì„±ëŠ¥/ìŠ¤ì¼€ì¤„ë§ ìš°ì„ )
- **TGI(Text Generation Inference)**: Hugging Face ìƒíƒœê³„ ì¤‘ì‹¬ + Docker ë°°í¬/ì˜µì…˜/ë©”íŠ¸ë¦­ì´ ì˜ ì •ë¦¬ëœ ìš´ì˜ ì¹œí™”í˜• ì„ íƒì§€ ([huggingface.co](https://huggingface.co/docs/text-generation-inference/en/quicktour))

ë˜ ì¤‘ìš”í•œ ë³€í™”: vLLMê³¼ TGI ëª¨ë‘ **OpenAI-compatible API**ë¥¼ ì œê³µí•´(ì˜ˆ: `/v1/chat/completions`) í´ë¼ì´ì–¸íŠ¸ ì½”ë“œë¥¼ ê±°ì˜ ì•ˆ ë°”ê¾¸ê³  ì—”ì§„ êµì²´ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. vLLMì€ `vllm serve`ë¡œ OpenAI í˜¸í™˜ ì„œë²„ë¥¼ ì§ì ‘ ë„ìš°ëŠ” íë¦„ì´ ë¬¸ì„œì—ì„œ ëª…í™•í•´ì¡ŒìŠµë‹ˆë‹¤(2026-01 ì—…ë°ì´íŠ¸). ([docs.vllm.ai](https://docs.vllm.ai/en/latest/serving/openai_compatible_server/?utm_source=openai))

---

## ğŸ”§ í•µì‹¬ ê°œë…
### 1) â€œì„œë¹™ ì—”ì§„â€ì´ í•˜ëŠ” ì¼: ìŠ¤ì¼€ì¤„ë§ + KV cache + ë°°ì¹˜
LLM ì¶”ë¡  ë¹„ìš©ì˜ í•µì‹¬ì€ **prefill(í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬)**ê³¼ **decode(í† í° ìƒì„±)**ì¸ë°, ì‹¤ì œ ì„œë¹™ì—ì„œëŠ” ìš”ì²­ì´ ì„ì´ë©´ì„œ ë‹¤ìŒ ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.

- ìš”ì²­ë§ˆë‹¤ ì…ë ¥ ê¸¸ì´/ì¶œë ¥ ê¸¸ì´ê°€ ë‹¤ë¦„ â†’ **GPUê°€ ë†€ê±°ë‚˜(OOM íšŒí”¼ ë•Œë¬¸ì—)**, ë°˜ëŒ€ë¡œ **í•œ ìš”ì²­ì´ ì „ì²´ë¥¼ ë§‰ëŠ”(head-of-line blocking)** ìƒí™© ë°œìƒ
- KV cacheê°€ ì»¤ì§ˆìˆ˜ë¡ ë©”ëª¨ë¦¬ ë‹¨í¸í™”/ë‚­ë¹„ â†’ batchë¥¼ í‚¤ìš°ê¸° ì–´ë ¤ì›€

vLLMì´ ê°•í•œ ì´ìœ ëŠ” (ëŒ€í‘œì ìœ¼ë¡œ) KV cacheë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” ì„¤ê³„(ì¼ë°˜ì ìœ¼ë¡œ PagedAttention ê³„ì—´ë¡œ ì•Œë ¤ì§) + ë™ì  ìŠ¤ì¼€ì¤„ë§ìœ¼ë¡œ **ë™ì‹œ ì‚¬ìš©ì ì¦ê°€ ì‹œ ì²˜ë¦¬ëŸ‰ì´ ì˜ ëŠ˜ì–´ë‚˜ëŠ” íŒ¨í„´**ì„ ë§Œë“¤ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ vLLMì´ Ollama ëŒ€ë¹„ ë†’ì€ TPS/ë‚®ì€ P99ë¥¼ ë³´ì˜€ë‹¤ëŠ” ë³´ê³ ê°€ ìˆìŠµë‹ˆë‹¤. ([developers.redhat.com](https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking))

### 2) OpenAI-compatible APIì˜ ì‹¤ë¬´ì  ì˜ë¯¸
ì„œë¹™ ì—”ì§„ì´ ë‹¬ë¼ë„ API ëª¨ì–‘ì„ ë§ì¶”ë©´ ë‹¤ìŒì´ ì‰¬ì›Œì§‘ë‹ˆë‹¤.

- ì•± ì„œë²„/í”„ë¡ íŠ¸/ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬(LangChain ë“±) ì½”ë“œ ìœ ì§€
- â€œë¡œì»¬(Ollama) â†’ ìŠ¤í…Œì´ì§•(TGI) â†’ í”„ë¡œë•ì…˜(vLLM)â€ ë‹¨ê³„ì  ì´ì „
- A/B í…ŒìŠ¤íŠ¸ë¡œ ì—”ì§„/ëª¨ë¸ êµì²´

vLLMì€ OpenAI Python clientë¥¼ ê·¸ëŒ€ë¡œ ì“°ëŠ” ì˜ˆì‹œë¥¼ ê³µì‹ ë¬¸ì„œì— ì œê³µí•©ë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/latest/serving/openai_compatible_server/?utm_source=openai))  
TGIë„ Dockerë¡œ ì‰½ê²Œ ë„ìš°ëŠ” Quick Tourì™€ ë°°í¬ í”Œë˜ê·¸ íë¦„ì´ ì˜ ì •ë¦¬ë¼ ìˆìŠµë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/text-generation-inference/en/quicktour))

### 3) ë¡œì»¬ ë°°í¬ vs ìš´ì˜ ë°°í¬ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì°¨ì´
- ë¡œì»¬: ì„¤ì¹˜ ë‹¨ìˆœì„±, ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ìºì‹œ, ê°œë°œ ìƒì‚°ì„±
- ìš´ì˜: ë©€í‹° GPU(sharding/tensor parallel), shared memory, ë©”íŠ¸ë¦­/íŠ¸ë ˆì´ì‹±, ë¡¤ë§ ì—…ë°ì´íŠ¸, ìì› ìƒí•œ(ìµœëŒ€ í† í°/ë°°ì¹˜)

vLLM Docker ë¬¸ì„œì—ì„œ **`--ipc=host` ë˜ëŠ” `--shm-size`**ë¥¼ ê°•ì¡°í•˜ëŠ”ë°, ì´ê±´ PyTorchê°€ í”„ë¡œì„¸ìŠ¤ ê°„ ê³µìœ  ë©”ëª¨ë¦¬ë¥¼ ì“°ê³ (íŠ¹íˆ tensor parallelì—ì„œ) ì—¬ê¸° ë§‰íˆë©´ ì„±ëŠ¥/ì•ˆì •ì„±ì´ ê¹¨ì§ˆ ìˆ˜ ìˆì–´ì„œ ìš´ì˜ì—ì„œ ìì£¼ ë°ŸëŠ” í•¨ì •ì…ë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/stable/deployment/docker/?utm_source=openai))

---

## ğŸ’» ì‹¤ì „ ì½”ë“œ
ì•„ë˜ëŠ” â€œí•œ ëŒ€ ì„œë²„ì— ë¡œì»¬/ìš´ì˜ ëª¨ë‘ ê°€ëŠ¥í•œ ìµœì†Œ êµ¬ì„±â€ì„ ëª©í‘œë¡œ ì¡ì•˜ìŠµë‹ˆë‹¤. (ëª¨ë¸ì€ ì˜ˆì‹œì´ë©°, í™˜ê²½ì— ë§ê²Œ ë°”ê¾¸ì„¸ìš”)

### 1) vLLM: Dockerë¡œ OpenAI ì„œë²„ ë„ìš°ê¸° (GPU)
```bash
# 1) HF í† í°(ê²Œì´í‹°ë“œ ëª¨ë¸ì´ë©´ í•„ìˆ˜)
export HF_TOKEN="YOUR_TOKEN"

# 2) vLLM OpenAI-compatible server ì‹¤í–‰
# --ipc=host ëŠ” shared memory ì´ìŠˆ íšŒí”¼(íŠ¹íˆ ë³‘ë ¬ ì¶”ë¡ ì—ì„œ ì¤‘ìš”)
docker run --runtime nvidia --gpus all \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  --env "HF_TOKEN=$HF_TOKEN" \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model NousResearch/Meta-Llama-3-8B-Instruct
```
ìœ„ ì»¤ë§¨ë“œ êµ¬ì¡°ëŠ” vLLM ê³µì‹ Docker ë°°í¬ ë¬¸ì„œì˜ í˜•íƒœë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¦…ë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/stable/deployment/docker/?utm_source=openai))

ì´ì œ í´ë¼ì´ì–¸íŠ¸ëŠ” OpenAI Python SDKë¡œ í˜¸ì¶œ:
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="token-abc123",  # vLLM ì„œë²„ ë„ìš¸ ë•Œ ì§€ì • ê°€ëŠ¥
)

resp = client.chat.completions.create(
    model="NousResearch/Meta-Llama-3-8B-Instruct",
    messages=[{"role": "user", "content": "vLLM ì„œë¹™ì—ì„œ ë³‘ëª©ì€ ë³´í†µ ì–´ë””ì„œ ìƒê²¨?"}],
    # vLLM ì „ìš© ì˜µì…˜ì€ extra_bodyë¡œ ë„˜ê¸°ëŠ” íŒ¨í„´(ì˜ˆ: top_k)
    extra_body={"top_k": 40},
)

print(resp.choices[0].message.content)
```
ì´ í˜¸ì¶œ íŒ¨í„´(íŠ¹íˆ `base_url` + `extra_body`)ì€ vLLM OpenAI í˜¸í™˜ ì„œë²„ ë¬¸ì„œì™€ ë™ì¼í•©ë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/latest/serving/openai_compatible_server/?utm_source=openai))

### 2) TGI: Dockerë¡œ ë¹ ë¥´ê²Œ ë„ìš°ê¸° (GPU)
```bash
model="teknium/OpenHermes-2.5-Mistral-7B"
volume="$PWD/data"

docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
  ghcr.io/huggingface/text-generation-inference:3.3.5 \
  --model-id $model
```
TGIëŠ” ê³µì‹ Quick Tourì—ì„œ ìœ„ì™€ ê°™ì€ â€œë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ ê°€ì¤‘ì¹˜ ì¬ë‹¤ìš´ë¡œë“œ ë°©ì§€â€ íŒ¨í„´ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/text-generation-inference/en/quicktour))

### 3) Ollama: ì„œë²„/í”„ë¡ì‹œ í™˜ê²½ì—ì„œ ìì£¼ ì“°ëŠ” ì„¤ì • í¬ì¸íŠ¸
OllamaëŠ” ë¡œì»¬ ì•±/ì—ì´ì „íŠ¸ ê°œë°œì—ì„œ íŠ¹íˆ ê°•ì ì´ ìˆëŠ”ë°, ìš´ì˜ ë„¤íŠ¸ì›Œí¬(í”„ë¡ì‹œ)ì—ì„œëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ê³µì‹ FAQëŠ” ëª¨ë¸ pullì´ HTTPS ê¸°ë°˜ì´ë¯€ë¡œ `HTTPS_PROXY`ë¥¼ ì“°ëŠ” ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ([docs.ollama.com](https://docs.ollama.com/faq))
```bash
# ì˜ˆ: í”„ë¡ì‹œ í™˜ê²½ì—ì„œ ëª¨ë¸ pullì´ í•„ìš”í•œ ê²½ìš°
export HTTPS_PROXY="https://proxy.example.com"

# (í™˜ê²½ì— ë”°ë¼) OLLAMA_HOST ë“±ë„ ì„¤ì • ê°€ëŠ¥
# export OLLAMA_HOST="0.0.0.0:11434"

ollama serve
```

---

## âš¡ ì‹¤ì „ íŒ
### 1) â€œì—”ì§„ ì„ íƒâ€ì„ ì„±ëŠ¥ì´ ì•„ë‹ˆë¼ ì›Œí¬ë¡œë“œë¡œ ê²°ì •í•˜ê¸°
- **ë™ì‹œ ì‚¬ìš©ì 1~ìˆ˜ ëª… + ë¡œì»¬ ê°œë°œ**: Ollamaê°€ ìƒì‚°ì„± ìµœê³ (ì„¤ì • ì ìŒ)
- **ë™ì‹œ ì‚¬ìš©ì ìˆ˜ì‹­~ìˆ˜ë°± + ë¹„ìš© ë¯¼ê°**: vLLM ìª½ì´ ìœ ë¦¬í•œ ê²½ìš°ê°€ ë§ìŒ(ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ vLLMì´ ê³ ë™ì‹œì„±ì—ì„œ ì²˜ë¦¬ëŸ‰/ì§€ì—°ì´ ê°•í•˜ê²Œ ë‚˜ì˜¤ëŠ” ì‚¬ë¡€ê°€ ë³´ê³ ë¨) ([developers.redhat.com](https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking))
- **HF ëª¨ë¸/ìš´ì˜ ê¸°ëŠ¥(ì˜µì…˜/ê°€ì´ë“œ/í•˜ë“œì›¨ì–´ ë‹¤ì–‘ì„±)**: TGI ë¬¸ì„œ/ê°€ì´ë“œ ì²´ê³„ê°€ íƒ„íƒ„ ([huggingface.co](https://huggingface.co/docs/text-generation-inference/en/quicktour))

### 2) vLLM ì»¨í…Œì´ë„ˆì—ì„œ ê°€ì¥ í”í•œ í•¨ì •: shared memory
- ì¦ìƒ: ë©€í‹° GPU/ë³‘ë ¬ ì¶”ë¡ ì—ì„œ ì„±ëŠ¥ ê¸‰ë½, í”„ë¡œì„¸ìŠ¤ í†µì‹  ë¬¸ì œ, ì˜ˆê¸°ì¹˜ ì•Šì€ hang
- ì²˜ë°©: `--ipc=host` ë˜ëŠ” `--shm-size`ë¥¼ ë°˜ë“œì‹œ ê²€í†   
vLLMì´ ê³µì‹ ë¬¸ì„œì—ì„œ ì´ í”Œë˜ê·¸ë¥¼ â€œPyTorchê°€ shared memoryë¥¼ ì“´ë‹¤â€ëŠ” ì´ìœ ë¡œ ì§ì ‘ ì–¸ê¸‰í•©ë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/stable/deployment/docker/?utm_source=openai))

### 3) â€œê¸°ë³¸ ìƒ˜í”Œë§ ê°’ì´ ì´ìƒí•˜ë‹¤â€ë©´ generation_configë¶€í„° ì˜ì‹¬
vLLMì€ ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ repoì˜ `generation_config.json`ì„ ì ìš©í•  ìˆ˜ ìˆì–´, ì„œë²„ì—ì„œ ìƒê°í•œ ê¸°ë³¸ temperature/top_pê°€ ë°”ë€ŒëŠ” ì¼ì´ ìƒê¹ë‹ˆë‹¤. í•„ìš”í•˜ë©´ ëŸ°ì¹˜ ì˜µì…˜ìœ¼ë¡œ ë™ì‘ì„ í†µì œí•˜ì„¸ìš”. ([docs.vllm.ai](https://docs.vllm.ai/en/latest/serving/openai_compatible_server/?utm_source=openai))

### 4) ê´€ì¸¡ì„±(ë©”íŠ¸ë¦­/íŠ¸ë ˆì´ì‹±)ì€ â€˜ë‚˜ì¤‘ì—â€™ê°€ ì•„ë‹ˆë¼ â€˜ì²˜ìŒì—â€™
- QPSê°€ ì˜¬ë¼ê°€ë©´ â€œëª¨ë¸ì´ ëŠë¦°ì§€, íê°€ ê¸´ì§€, prefillì´ ë§‰íˆëŠ”ì§€â€ë¥¼ êµ¬ë¶„í•´ì•¼ í•©ë‹ˆë‹¤.
- vLLMì€ OpenAI í˜¸í™˜ ì„œë²„ì—ì„œ metrics/tracing ê´€ë ¨ ì˜µì…˜ì´ ê³„ì† í™•ì¥ë˜ëŠ” íë¦„ì´ ë³´ì…ë‹ˆë‹¤(ë²„ì „ë³„ hidden metrics, OTLP traces ë“±). ([docs.vllm.ai](https://docs.vllm.ai/en/v0.8.4/serving/openai_compatible_server.html?utm_source=openai))  
ìš´ì˜ì´ë¼ë©´ Prometheus + OTLP(OpenTelemetry)ê¹Œì§€ ì—¼ë‘ì— ë‘ê³  ë°°í¬ íŒŒì´í”„ë¼ì¸ì„ ì¡ëŠ” ê²Œ ì¥ê¸°ì ìœ¼ë¡œ ì‹¸ê²Œ ë¨¹í™ë‹ˆë‹¤.

---

## ğŸš€ ë§ˆë¬´ë¦¬
ì •ë¦¬í•˜ë©´, 2026ë…„ 2ì›” ì‹œì ì˜ ì‹¤ì „ ë°°í¬ ì „ëµì€ ì´ë ‡ê²Œ ê°€ì ¸ê°€ë©´ ì‹œí–‰ì°©ì˜¤ê°€ ì¤„ì–´ë“­ë‹ˆë‹¤.

- **ë¡œì»¬ ê°œë°œ/PoC**: Ollamaë¡œ ë¹ ë¥´ê²Œ ë°˜ë³µ
- **ìš´ì˜ ë°°í¬(ê³ ë™ì‹œì„±/ë¹„ìš© ìµœì í™”)**: vLLM(OpenAI-compatible server + Docker) ì¤‘ì‹¬, `--ipc=host` ê°™ì€ ìš´ì˜ í”Œë˜ê·¸ë¥¼ ì´ˆê¸°ì— í‘œì¤€í™” ([docs.vllm.ai](https://docs.vllm.ai/en/stable/deployment/docker/?utm_source=openai))
- **HF ìƒíƒœê³„/ë¬¸ì„œí™”/ë°°í¬ ê°€ì´ë“œ ì¤‘ì‹œ**: TGI Docker ê¸°ë°˜ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì„œë¹™í•˜ê³ , í•„ìš” ì‹œ vLLMë¡œ ì´ì „ ([huggingface.co](https://huggingface.co/docs/text-generation-inference/en/quicktour))

ë‹¤ìŒ í•™ìŠµ ì¶”ì²œ:
- vLLM: OpenAI-compatible serverì˜ ì„¸ë¶€ ì˜µì…˜(ìŠ¤ì¼€ì¤„ë§/ë©”íŠ¸ë¦­/íŠ¸ë ˆì´ì‹±)ê³¼ ë©€í‹° GPU ì „ëµ
- TGI: CLI ì˜µì…˜(ìƒ¤ë”©/quantization/ë°°ì¹˜)ê³¼ Prometheus ëª¨ë‹ˆí„°ë§
- ê³µí†µ: ë¶€í•˜í…ŒìŠ¤íŠ¸(ë™ì‹œì„±ë³„ TTFT/P99) ìŠ¤í¬ë¦½íŠ¸ í‘œì¤€í™” â†’ ì—”ì§„ êµì²´ ì‹œ â€œê°â€ì´ ì•„ë‹ˆë¼ â€œìˆ˜ì¹˜â€ë¡œ ê²°ì •í•˜ê¸°
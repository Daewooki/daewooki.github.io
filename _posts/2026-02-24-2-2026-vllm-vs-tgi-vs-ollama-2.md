---
title: "2ì›” 2026 ê¸°ì¤€: vLLM vs TGI vs Ollama, â€œì–´ë–»ê²Œâ€ ë°°í¬í•˜ê³  â€œì™œâ€ ê·¸ë ‡ê²Œ íŠœë‹í•˜ëŠ”ê°€"
date: 2026-02-24 02:48:34 +0900
categories: [AI, MLOps]
tags: [ai, mlops, trend, 2026-02]
---

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7990TVG7C7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7990TVG7C7');
</script>

## ë“¤ì–´ê°€ë©°
LLM ì„œë¹™ì„ ìš´ì˜í•´ë³´ë©´ ê³§ ë‘ ê°€ì§€ ë²½ì— ë¶€ë”ªí™ë‹ˆë‹¤. **(1) GPU ë©”ëª¨ë¦¬(KV cache) í•œê³„**ì™€ **(2) ë™ì‹œì„±/ì§€ì—°ì‹œê°„(SLO) í•œê³„**ì…ë‹ˆë‹¤. ëª¨ë¸ ê°€ì¤‘ì¹˜ë³´ë‹¤ KV cacheê°€ ë” ë¹¨ë¦¬ ë©”ëª¨ë¦¬ë¥¼ ì¡ì•„ë¨¹ê³ , ìš”ì²­ì´ ì¡°ê¸ˆë§Œ ëª°ë ¤ë„ prefill êµ¬ê°„ì—ì„œ ì§€ì—°ì´ ê¸‰ì¦í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ 2026ë…„ 2ì›” ê¸°ì¤€ ì‹¤ë¬´ì—ì„œëŠ” â€œì„œë¹™ ì—”ì§„ ì„ íƒâ€ì´ ê³§ â€œì¸í”„ë¼ ë¹„ìš©ê³¼ ì„±ëŠ¥ì˜ ìš´ëª…â€ì´ ë©ë‹ˆë‹¤.

ìš”ì•½í•˜ë©´:
- **vLLM**: ê³ ì„±ëŠ¥/ê³ ì²˜ë¦¬ëŸ‰(continuous batching, PagedAttention) + OpenAI-compatible APIë¡œ â€œí”„ë¡œë•ì…˜ ì„œë¹™ ê¸°ë³¸ê°’â€ì— ê°€ê¹Œì›€ ([vllm.ai](https://vllm.ai/?utm_source=openai))  
- **TGI (Text Generation Inference)**: ê¸°ëŠ¥ì€ ê°•í•˜ì§€ë§Œ, Hugging Face ë¬¸ì„œ ê¸°ì¤€ **2025-12-11ë¶€í„° maintenance mode**ë¼ ì¥ê¸° ì „ëµì€ ì¬ê²€í†  í•„ìš” ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))  
- **Ollama**: ë¡œì»¬/ì—£ì§€/ê°œë°œì ê²½í—˜(DX) ìµœê°•. ëŒ€ì‹  ê³ ë¶€í•˜ GPU ì„œë¹™ë³´ë‹¨ â€œë‚´ PC/ì‚¬ë‚´ ì›Œí¬ìŠ¤í…Œì´ì…˜ ë°°í¬â€ì— ìµœì  ([docs.ollama.com](https://docs.ollama.com/faq?utm_source=openai))  

---

## ğŸ”§ í•µì‹¬ ê°œë…
### 1) Continuous batching: â€œë°°ì¹˜ë¥¼ ê³ ì •í•˜ì§€ ë§ê³ , íë¥´ëŠ” ìš”ì²­ì„ ê³„ì† ë¬¶ì–´ë¼â€
ì „í†µì ì¸ ë°°ì¹˜ëŠ” â€œNê°œ ëª¨ì´ë©´ ì‹¤í–‰â€ì´ë¼ tail latencyê°€ ë§ê°€ì§€ê¸° ì‰½ìŠµë‹ˆë‹¤. vLLM/TGIë¥˜ëŠ” **in-flight requestë¥¼ ìŠ¤ì¼€ì¤„ë§**í•´ GPUë¥¼ ê³„ì† ì±„ìš°ëŠ” ë°©ì‹(continuous batching)ì„ ì”ë‹ˆë‹¤. íŠ¹íˆ streamingê³¼ ê¶í•©ì´ ì¢‹ìŠµë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))  

### 2) KV cacheê°€ ì§„ì§œ ë³‘ëª©ì´ë‹¤ (ê·¸ë¦¬ê³  PagedAttentionì˜ ì˜ë¯¸)
LLM inferenceëŠ” í† í°ì„ í•œ ê°œì”© ìƒì„±í• ìˆ˜ë¡ ê³¼ê±° í† í°ì˜ K/Vë¥¼ ì¬ì‚¬ìš©(KV cache)í•©ë‹ˆë‹¤. ë¬¸ì œëŠ” ì»¨í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ KV cacheê°€ ê¸°í•˜ê¸‰ìˆ˜ë¡œ ì»¤ì ¸ **ë™ì‹œì„±(concurrency)**ì„ ê°‰ì•„ë¨¹ëŠ”ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.  
vLLMì€ **PagedAttention**ìœ¼ë¡œ KV cacheë¥¼ â€œOSì˜ pagingì²˜ëŸ¼â€ ìª¼ê°œì„œ ê´€ë¦¬í•´ ë‚­ë¹„ë¥¼ ì¤„ì´ê³ , ê²°ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ì„ ëŒì–´ì˜¬ë¦½ë‹ˆë‹¤(ê³µì‹ í™ˆí˜ì´ì§€ì—ì„œë„ í•µì‹¬ ê°€ì¹˜ë¡œ ê°•ì¡°). ([vllm.ai](https://vllm.ai/?utm_source=openai))  

### 3) OpenAI-compatible APIëŠ” â€œí”„ë¡ì‹œ/í‘œì¤€í™” ë ˆì´ì–´â€
2026ë…„ í˜„ì¬ ì„œë¹™ ì—”ì§„ì€ ë°”ë€Œì–´ë„ ì•± ì½”ë“œëŠ” ìœ ì§€í•˜ë ¤ë©´ **/v1/chat/completions** ê°™ì€ OpenAI ìŠ¤í™ í˜¸í™˜ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. vLLMì€ OpenAI-compatible serverë¥¼ ê³µì‹ ë¬¸ì„œë¡œ ì œê³µí•˜ê³ , chat template ì´ìŠˆ(ëª¨ë¸ tokenizerì˜ chat_template ìœ ë¬´)ê°€ ì‹¤ë¬´ í•¨ì • í¬ì¸íŠ¸ë¡œ ë¬¸ì„œì— ëª…ì‹œë¼ ìˆìŠµë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html?utm_source=openai))  

### 4) TGIì˜ í˜„ì¬ í¬ì§€ì…˜(ì¤‘ìš”)
Hugging Face ë¬¸ì„œì—ì„œ **TGIëŠ” 2025-12-11ë¶€í„° maintenance mode**ë¼ê³  ëª…ì‹œí•©ë‹ˆë‹¤. ì¦‰, ì§€ê¸ˆ ë‹¹ì¥ êµ´ë¦´ ìˆ˜ëŠ” ìˆì–´ë„ â€œ2~3ë…„ ìš´ì˜í•  ì„œë¹™ í‘œì¤€â€ìœ¼ë¡œëŠ” vLLM/SGLang ê°™ì€ ëŒ€ì•ˆì„ ê²€í† í•˜ëŠ” ê²Œ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))  

---

## ğŸ’» ì‹¤ì „ ì½”ë“œ
ì•„ë˜ëŠ” â€œí•œ ëŒ€ ì„œë²„ì— ë¡œì»¬ ë°°í¬â€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, **vLLM / TGI / Ollama**ë¥¼ ê°ê° ë„ìš°ê³ (OpenAI í˜¸í™˜ ë˜ëŠ” HTTP í˜¸ì¶œ) ë™ì¼í•œ í´ë¼ì´ì–¸íŠ¸ ì½”ë“œë¡œ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```bash
# 1) vLLM: OpenAI-compatible server (ê¶Œì¥: í”„ë¡œë•ì…˜ ì§€í–¥)
# - ëª¨ë¸ì´ chat_templateì„ ì•ˆ ê°–ê³  ìˆìœ¼ë©´ --chat-templateë¡œ ì§€ì •í•´ì•¼ í•¨(í•¨ì • í¬ì¸íŠ¸)
# - ë©€í‹° GPUë©´ --tensor-parallel-size ì¡°ì •
pip install -U vllm

vllm serve NousResearch/Meta-Llama-3-8B-Instruct \
  --host 0.0.0.0 --port 8000 \
  --max-model-len 8192 \
  --enable-offline-docs
```

```bash
# (ì˜µì…˜) vLLM ë©€í‹° GPU ìŠ¤ì¼€ì¼ë§
# ë‹¨ì¼ ë…¸ë“œì—ì„œ ëª¨ë¸ì´ 1ì¥ì— ì•ˆ ë“¤ì–´ê°€ë©´ tensor parallelì´ 1ìˆœìœ„
vllm serve <MODEL_ID> \
  --tensor-parallel-size 4 \
  --host 0.0.0.0 --port 8000
```

```bash
# 2) TGI: Dockerë¡œ ë¹ ë¥´ê²Œ ë„ìš°ê¸° (ë‹¨, HF ë¬¸ì„œìƒ maintenance mode ìœ ì˜)
docker run --gpus all --shm-size 1g \
  -p 8080:80 \
  -v ~/.cache/huggingface:/data \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id HuggingFaceTB/SmolLM2-360M-Instruct
```

```bash
# 3) Ollama: ë¡œì»¬/ì‚¬ë‚´ ê°œë°œ í™˜ê²½ì—ì„œ â€œê°€ì¥ ê°„ë‹¨â€
# ë„¤íŠ¸ì›Œí¬ì— ë…¸ì¶œí•˜ë ¤ë©´ OLLAMA_HOSTë¥¼ 0.0.0.0:11434ë¡œ
# (Linux systemd ì„œë¹„ìŠ¤ë¼ë©´ systemctl editë¡œ Environment ì¶”ê°€)
# ì˜ˆ: Environment="OLLAMA_HOST=0.0.0.0:11434"
```

```python
# ê³µí†µ í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸:
# vLLMì€ OpenAI python clientë¡œ ë°”ë¡œ í˜¸ì¶œ(ì„œë²„ê°€ /v1 ì œê³µ)
# TGIë„ OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì œê³µí•  ìˆ˜ ìˆìœ¼ë‚˜(í™˜ê²½/ë²„ì „/ì„¤ì •ì— ë”°ë¼),
# ì—¬ê¸°ì„œëŠ” "vLLM OpenAI í˜¸í™˜"ì„ ê¸°ì¤€ ì˜ˆì œë¡œ ë‘ .

from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",  # vLLM
    api_key="local-token"                 # ë¡œì»¬ì´ë©´ ì˜ë¯¸ë§Œ ì±„ì›€
)

resp = client.chat.completions.create(
    model="NousResearch/Meta-Llama-3-8B-Instruct",
    messages=[
        {"role": "system", "content": "You are a concise assistant."},
        {"role": "user", "content": "LLM servingì—ì„œ KV cacheê°€ ë³‘ëª©ì¸ ì´ìœ ë¥¼ 3ì¤„ë¡œ ì„¤ëª…í•´ì¤˜."},
    ],
    temperature=0.2,
)

print(resp.choices[0].message.content)
```

---

## âš¡ ì‹¤ì „ íŒ
1) **chat_template ì—†ëŠ” ëª¨ë¸ì€ â€œì„œë²„ê°€ ì£½ëŠ” ê²Œ ì•„ë‹ˆë¼, ìš”ì²­ì´ ì—ëŸ¬â€ë¡œ í„°ì§„ë‹¤**  
vLLM ë¬¸ì„œì—ì„œ chat templateì„ tokenizerê°€ ì œê³µí•´ì•¼ í•˜ê³ , ì—†ìœ¼ë©´ `--chat-template`ë¡œ ì§€ì •í•˜ë¼ê³  ëª» ë°•ìŠµë‹ˆë‹¤. ìš´ì˜ ì¤‘ â€œíŠ¹ì • ëª¨ë¸ë§Œ 400/500â€ ë‚˜ëŠ” ì „í˜•ì ì¸ ì›ì¸ì…ë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html?utm_source=openai))  

2) **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ëŠ” â€˜í’ˆì§ˆâ€™ì´ ì•„ë‹ˆë¼ â€˜ë™ì‹œì„±â€™ê³¼ íŠ¸ë ˆì´ë“œì˜¤í”„**  
TGI ë¬¸ì„œë„ ê¸´ contextë¥¼ ê·¸ëŒ€ë¡œ ì—´ë©´ ë™ì‹œ ìš”ì²­ ìˆ˜ê°€ ê¸‰ê°í•  ìˆ˜ ìˆê³ , max input lengthë¥¼ ë‚®ì¶° ë™ì‹œì„±ì„ í™•ë³´í•˜ë¼ëŠ” ì‹ì˜ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìš´ì˜ì—ì„œëŠ” â€œ128k ê°€ëŠ¥â€ë³´ë‹¤ â€œSLO ë§Œì¡±â€ì´ ìš°ì„ ì…ë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))  

3) **ë©€í‹° GPU í™•ì¥ì€ vLLMì—ì„œ tensor parallel â†’ (í•„ìš”ì‹œ) pipeline parallel ìˆœì„œë¡œ**  
ë‹¨ì¼ ë…¸ë“œ ë©€í‹° GPUëŠ” tensor parallelì´ ê¸°ë³¸ ì„ íƒì´ê³ , ë…¸ë“œë¥¼ ë„˜ì–´ê°€ë©´ tensor+pipeline ì¡°í•©ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì´ ê¸°ì¤€ì„ ëª¨ë¥´ë©´ ê´œíˆ ë¶„ì‚° êµ¬ì„±ì„ ë¨¼ì € í•˜ë‹¤ ë””ë²„ê¹… ì§€ì˜¥ì´ ì—´ë¦½ë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/v0.14.0/serving/parallelism_scaling/?utm_source=openai))  

4) **Ollamaë¥¼ ì™¸ë¶€ì— ì—´ ë•ŒëŠ” OLLAMA_HOST + ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œë¥¼ ì„¸íŠ¸ë¡œ**
OllamaëŠ” ê¸°ë³¸ ë°”ì¸ë”©ì´ `127.0.0.1:11434`ë¼ ì›ê²© ì ‘ì†ì´ ì•ˆ ë©ë‹ˆë‹¤. ë¬¸ì„œì— ë‚˜ì˜¨ ëŒ€ë¡œ `OLLAMA_HOST=0.0.0.0:11434`ë¡œ ë°”ê¾¸ê³ , í•„ìš”í•˜ë©´ Nginxë¡œ í”„ë¡ì‹œë¥¼ ë‘¬ì„œ ì ‘ê·¼ ì œì–´/ë¡œê·¸/ì¸ì¦ì„ ë¶™ì´ëŠ” ê²Œ ì•ˆì „í•©ë‹ˆë‹¤. ([docs.ollama.com](https://docs.ollama.com/faq?utm_source=openai))  

5) **TGIëŠ” â€œì§€ê¸ˆ ëŒë¦´ ìˆ˜ ìˆìŒâ€ê³¼ â€œì¥ê¸° í‘œì¤€â€ì„ ë¶„ë¦¬í•´ì„œ íŒë‹¨**
HF ê³µì‹ ë¬¸ì„œì— maintenance modeê°€ ëª…ì‹œëœ ì´ìƒ, ì‹ ê·œ êµ¬ì¶•ì€ vLLM ìª½ìœ¼ë¡œ ê¸°ìš¸ê³ (íŠ¹íˆ Inference Endpointsë„ vLLM ëŒ€ì•ˆì„ ê¶Œì¥), TGIëŠ” ê¸°ì¡´ ìš´ì˜ë¶„ ìœ ì§€/ë§ˆì´ê·¸ë ˆì´ì…˜ ê´€ì ìœ¼ë¡œ ë³´ëŠ” ê²Œ í•©ë¦¬ì ì…ë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))  

---

## ğŸš€ ë§ˆë¬´ë¦¬
- **í”„ë¡œë•ì…˜ GPU ì„œë¹™ ì¸í”„ë¼**: vLLMì´ ì‚¬ì‹¤ìƒ 1ìˆœìœ„(continuous batching + PagedAttention + OpenAI-compatible). ([vllm.ai](https://vllm.ai/?utm_source=openai))  
- **ê¸°ì¡´ TGI ìš´ì˜**: 2025-12-11 ì´í›„ maintenance modeì´ë¯€ë¡œ, ì‹ ê·œëŠ” ì‹ ì¤‘/ë§ˆì´ê·¸ë ˆì´ì…˜ í”Œëœ í•„ìˆ˜. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))  
- **ë¡œì»¬ ë°°í¬/ì‚¬ë‚´ ê°œë°œ í‘œì¤€**: OllamaëŠ” ì„¤ì •ì´ ë‹¨ìˆœí•˜ê³ (íŠ¹íˆ OLLAMA_HOSTë¡œ ë…¸ì¶œ), íŒ€ ìƒì‚°ì„±ì„ ì˜¬ë¦¬ê¸° ì¢‹ìŠµë‹ˆë‹¤. ([docs.ollama.com](https://docs.ollama.com/faq?utm_source=openai))  

ë‹¤ìŒ í•™ìŠµ ì¶”ì²œ:
1) vLLMì˜ OpenAI-compatible server ì˜µì…˜(í…œí”Œë¦¿, extra params, ë©€í‹°ëª¨ë‹¬ ì…ë ¥) ë¬¸ì„œ ì •ë… ([docs.vllm.ai](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html?utm_source=openai))  
2) â€œì»¨í…ìŠ¤íŠ¸ ê¸¸ì´/ë™ì‹œì„±/ë©”ëª¨ë¦¬â€ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ SLO ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì¹˜í™”(ë¶€í•˜í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ + Prometheus ì§€í‘œ)  
3) (ì¡°ì§ ê·œëª¨ê°€ í¬ë©´) Ray Serve LLM ê°™ì€ ìƒìœ„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ë„ ê²€í†  ([docs.vllm.ai](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html?utm_source=openai))
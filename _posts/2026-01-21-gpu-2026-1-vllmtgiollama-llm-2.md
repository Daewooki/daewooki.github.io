---
title: "ë¡œì»¬ë¶€í„° ë©€í‹° GPUê¹Œì§€: 2026ë…„ 1ì›” ê¸°ì¤€ vLLMÂ·TGIÂ·Ollama LLM ì„œë¹™ ë°°í¬/ìµœì í™” ì‹¤ì „ ê°€ì´ë“œ"
date: 2026-01-21 02:25:52 +0900
categories: [AI, MLOps]
tags: [ai, mlops, trend, 2026-01]
---

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7990TVG7C7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7990TVG7C7');
</script>

## ë“¤ì–´ê°€ë©°
2026ë…„ 1ì›” ì‹œì ì˜ LLM ì„œë¹™ì€ â€œëª¨ë¸ì„ ë„ìš´ë‹¤â€ë¥¼ ë„˜ì–´, **ë™ì‹œì„±(throughput)**Â·**TTFT(Time To First Token)**Â·**KV cache ë©”ëª¨ë¦¬ íš¨ìœ¨**Â·**API í˜¸í™˜ì„±(OpenAI compatible)**Â·**ìš´ì˜ í¸ì˜ì„±(ê´€ì¸¡/ë¡¤ë§ì—…ë°ì´íŠ¸)**ì´ ì„±íŒ¨ë¥¼ ê°€ë¦…ë‹ˆë‹¤. íŠ¹íˆ ì‚¬ë‚´ë§/ì˜¨í”„ë ˜ í™˜ê²½ì—ì„œ â€œí´ë¼ìš°ë“œ API ì—†ì´ ë¡œì»¬ ë°°í¬â€ ìš”êµ¬ê°€ ì»¤ì§€ë©´ì„œ vLLM, TGI(Text Generation Inference), Ollamaê°€ ìì£¼ ë¹„êµë©ë‹ˆë‹¤.

í¥ë¯¸ë¡œìš´ ë³€í™”ë„ ìˆìŠµë‹ˆë‹¤. Hugging Face ë¬¸ì„œì— ë”°ë¥´ë©´ **TGIëŠ” 2025-12-11 ê¸°ì¤€ maintenance mode**ë¡œ ì „í™˜ë˜ì—ˆê³ , Inference Endpointsì—ì„œëŠ” ëŒ€ì•ˆìœ¼ë¡œ vLLM/SGLangë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))  
ì¦‰, **ì‹ ê·œ êµ¬ì¶•ì˜ ê¸°ë³¸ê°’ì€ vLLM**, ë‹¤ë§Œ â€œì´ë¯¸ TGIë¡œ êµ³ì–´ì§„ ìš´ì˜â€ì´ë‚˜ â€œíŠ¹ì • ê¸°ëŠ¥/ìŠ¤íƒâ€ ë•Œë¬¸ì— TGIë¥¼ ìœ ì§€í•˜ëŠ” íŒ€ë„ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤.

---

## ğŸ”§ í•µì‹¬ ê°œë…
### 1) vLLM: PagedAttention + Continuous batching
vLLMì´ ê°•í•œ ì´ìœ ëŠ” **PagedAttention ê¸°ë°˜ KV cache ê´€ë¦¬**ì™€ **continuous batching**ì…ë‹ˆë‹¤. KV cacheë¥¼ ê³ ì • ë¸”ë¡ ë‹¨ìœ„ë¡œ ê´€ë¦¬í•´ ë‹¨í¸í™”ë¥¼ ì¤„ì´ê³ , ì‹¤í–‰ ì¤‘ì¸ ìš”ì²­ë“¤ì„ ë™ì ìœ¼ë¡œ ë¬¶ì–´ GPUë¥¼ ìµœëŒ€í•œ íƒœì›ë‹ˆë‹¤(ë™ì‹œ ìš”ì²­ì´ ëŠ˜ìˆ˜ë¡ ê°•í•´ì§). ([marktechpost.com](https://www.marktechpost.com/2025/11/07/comparing-the-top-6-inference-runtimes-for-llm-serving-in-2025/?utm_source=openai))  
ë˜í•œ OpenAI-compatible ì„œë²„ ëª¨ë“œë¥¼ ê³µì‹ ì§€ì›í•˜ë©°, `--tensor-parallel-size`, `--pipeline-parallel-size`, `--gpu-memory-utilization`, `--enable-prefix-caching` ê°™ì€ ìš´ì˜ í•µì‹¬ ì˜µì…˜ì´ ì˜ ì •ë¦¬ë¼ ìˆìŠµë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/v0.8.3/serving/openai_compatible_server.html?utm_source=openai))

ì¶”ê°€ë¡œ, vLLMì˜ **Automatic Prefix Caching**ëŠ” â€œê°™ì€ prefixë¥¼ ê°€ì§„ ìš”ì²­â€ì—ì„œ í”„ë¡¬í”„íŠ¸ prefillì„ ì¬ì‚¬ìš©í•´ ë¹„ìš©ì„ ì¤„ì…ë‹ˆë‹¤. vLLM v1 ì„¤ê³„ ë¬¸ì„œì—ì„œ í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ KV ë¸”ë¡ì„ ìºì‹œ/ì¬ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/v0.10.1.1/design/prefix_caching.html?utm_source=openai))

### 2) TGI: ì„±ìˆ™í•œ í”„ë¡œë•ì…˜ ê¸°ëŠ¥ vs maintenance mode
TGIëŠ” Rust/Python ê¸°ë°˜ì˜ ê³ ì„±ëŠ¥ ì„œë¹™ ì—”ì§„ìœ¼ë¡œ, **streaming**, **continuous batching**, **OpenAI-compatible `/v1/chat/completions`**, ë©”íŠ¸ë¦­/íŠ¸ë ˆì´ì‹± ë“±ì„ ê°•ì ìœ¼ë¡œ ë‚´ì„¸ì›ë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))  
ë‹¤ë§Œ ì§€ê¸ˆì€ maintenance modeì´ë¯€ë¡œ(ì‹ ê·œ ê¸°ëŠ¥ë³´ë‹¤ëŠ” ìœ ì§€ë³´ìˆ˜ ìœ„ì£¼) â€œì¥ê¸° ë¡œë“œë§µâ€ ê´€ì ì—ì„œëŠ” vLLMë¡œ ì´ë™ì´ ìì—°ìŠ¤ëŸ¬ìš´ ì„ íƒì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))

### 3) Ollama: ë¡œì»¬ ê°œë°œì ê²½í—˜(Developer UX) ìµœê°•, ê³ ë™ì‹œì„±ì€ íŠœë‹ í•„ìš”
OllamaëŠ” **ë¡œì»¬ ì‹¤í–‰/ëª¨ë¸ ê´€ë¦¬/ê°„ë‹¨í•œ HTTP API**ì— ìµœì í™”ëœ UXë¥¼ ì œê³µí•©ë‹ˆë‹¤. APIëŠ” ê¸°ë³¸ì ìœ¼ë¡œ `http://localhost:11434/api`ë¡œ ë…¸ì¶œë˜ë©° `/api/chat`, `/api/generate`ë¥¼ ë°”ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ([docs.ollama.com](https://docs.ollama.com/api/introduction?utm_source=openai))  
ë‹¤ë§Œ â€œê¸°ë³¸ ì„¤ì •â€ì€ ë‹¨ì¼ ì‚¬ìš©ì ì„±ê²©ì´ ê°•í•´ ë™ì‹œì„±ì—ì„œ vLLM ëŒ€ë¹„ ë¶ˆë¦¬í•  ìˆ˜ ìˆê³ , ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ vLLMì´ ë†’ì€ concurrencyì—ì„œ throughput/TTFT ë©´ì—ì„œ ìš°ì„¸í•œ ê²°ê³¼ê°€ ë³´ê³ ë©ë‹ˆë‹¤. ([developers.redhat.com](https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking?utm_source=openai))  
ë˜í•œ í™˜ê²½ë³€ìˆ˜ ë™ì‘/ë¬¸ì„œí™”ê°€ ë²„ì „ì— ë”°ë¼ í˜¼ì„ ì´ ìƒê¸¸ ìˆ˜ ìˆì–´(ì˜ˆ: `OLLAMA_NUM_PARALLEL` ì´ìŠˆ) ìš´ì˜ ì‹œ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤. ([github.com](https://github.com/ollama/ollama/issues/5722?utm_source=openai))

---

## ğŸ’» ì‹¤ì „ ì½”ë“œ
ì•„ë˜ëŠ” â€œí•œ ëŒ€ ì„œë²„ì—ì„œ ë¡œì»¬ ë°°í¬â€ ê¸°ì¤€ìœ¼ë¡œ, **vLLM / TGI / Ollama**ë¥¼ ê°ê° ë„ìš°ê³  í´ë¼ì´ì–¸íŠ¸ì—ì„œ í˜¸ì¶œí•˜ëŠ” ì‹¤í–‰ ì˜ˆì œì…ë‹ˆë‹¤.

```bash
# 1) vLLM (OpenAI-compatible) - ë‹¨ì¼ ë…¸ë“œ/ë‹¨ì¼ GPU ì˜ˆì‹œ
# í•µì‹¬: gpu ë©”ëª¨ë¦¬ ìƒí•œ, prefix caching, tensor parallel(ë©€í‹° GPUë©´ -tp ì¡°ì •)
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Meta-Llama-3.1-8B-Instruct \
  --host 0.0.0.0 --port 8000 \
  --gpu-memory-utilization 0.90 \
  --enable-prefix-caching \
  --tensor-parallel-size 1
#  -gpu-memory-utilization: KV cacheê¹Œì§€ í¬í•¨í•œ ë©”ëª¨ë¦¬ í”Œëœì— í° ì˜í–¥
#  -enable-prefix-caching: ë™ì¼ prefix ë°˜ë³µ í˜¸ì¶œ(íˆ´/ì—ì´ì „íŠ¸/í…œí”Œë¦¿)ì—ì„œ ë¹„ìš© ì ˆê°
#  -tensor-parallel-size: ë©€í‹° GPUë©´ ë³´í†µ GPU ê°œìˆ˜ì— ë§ì¶° 2/4/8 ë“±ìœ¼ë¡œ ì„¤ì • ([docs.vllm.ai](https://docs.vllm.ai/en/v0.8.3/serving/openai_compatible_server.html?utm_source=openai))


# 2) TGI - Dockerë¡œ ë¹ ë¥´ê²Œ ë„ìš°ê¸°
# (maintenance modeì´ì§€ë§Œ, ê¸°ì¡´ ìš´ì˜/í˜¸í™˜ì„± ë•Œë¬¸ì— ì—¬ì „íˆ ì“°ëŠ” ê²½ìš°ê°€ ë§ìŒ)
model=HuggingFaceH4/zephyr-7b-beta
volume=$PWD/data

docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
  ghcr.io/huggingface/text-generation-inference:3.3.5 \
  --model-id $model
# ì´í›„ OpenAI-compatible endpointë„ ì œê³µ(/v1/chat/completions) ([github.com](https://github.com/huggingface/text-generation-inference?utm_source=openai))


# 3) Ollama - ì„œë²„ ë°”ì¸ë”©ì„ ì™¸ë¶€ë¡œ ì—´ê³ (í•„ìš” ì‹œ) API í˜¸ì¶œ
# Linux(systemd) í™˜ê²½ì´ë©´ ì„œë¹„ìŠ¤ì— í™˜ê²½ë³€ìˆ˜ë¡œ OLLAMA_HOST ì„¤ì • ê°€ëŠ¥ ([docs.ollama.com](https://docs.ollama.com/faq?utm_source=openai))
export OLLAMA_HOST=0.0.0.0:11434
ollama serve

# Ollama Chat API í˜¸ì¶œ (streaming ê¸°ë³¸)
curl http://localhost:11434/api/chat -d '{
  "model": "gemma3",
  "messages": [{"role":"user","content":"KV cacheê°€ ì™œ ì¤‘ìš”í•œê°€?"}]
}'
# /api/chat ìŠ¤í™ì€ ê³µì‹ ë¬¸ì„œì— ì •ë¦¬ ([docs.ollama.com](https://docs.ollama.com/api/chat?utm_source=openai))
```

---

## âš¡ ì‹¤ì „ íŒ
1) **â€œë™ì‹œì„±â€ ëª©í‘œê°€ ìˆìœ¼ë©´ vLLMì„ ê¸°ë³¸ê°’ìœ¼ë¡œ**
- ë†’ì€ concurrencyì—ì„œ vLLMì€ dynamic scheduling/continuous batchingìœ¼ë¡œ throughputì´ ì˜ ìŠ¤ì¼€ì¼í•©ë‹ˆë‹¤. ë°˜ë©´ OllamaëŠ” ê¸°ë³¸ ì„¤ì •ì´ ë³‘ë ¬ ì²˜ë¦¬ì— ë³´ìˆ˜ì ì´ë¼ íŠœë‹ ì—†ì´ëŠ” ê¸ˆë°© plateauê°€ ì˜µë‹ˆë‹¤. ([developers.redhat.com](https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking?utm_source=openai))  
- ì‚¬ë‚´ ì„œë¹„ìŠ¤(ì—¬ëŸ¬ íŒ€/ë´‡/ì—ì´ì „íŠ¸)ê°€ í•œ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ê³µìœ í•œë‹¤ë©´, vLLMì´ ìš´ì˜ ì•ˆì •ì„±/ì„±ëŠ¥ ì˜ˆì¸¡ì´ ì‰¬ìš´ í¸ì…ë‹ˆë‹¤.

2) **Prefix cachingì€ â€œì—ì´ì „íŠ¸/í…œí”Œë¦¿ ê¸°ë°˜â€ ì›Œí¬ë¡œë“œì—ì„œ ì²´ê°ì´ í¼**
- ë™ì¼í•œ system prompt, ë™ì¼í•œ tool ì„¤ëª…, ë™ì¼í•œ ì •ì±… ë¬¸êµ¬ê°€ ë°˜ë³µë˜ëŠ” ì„œë¹„ìŠ¤ëŠ” prefix cachingì´ â€œê±°ì˜ ê³µì§œ ì ìˆ˜â€ê°€ ë©ë‹ˆë‹¤. vLLMì€ í•´ì‹œ ê¸°ë°˜ KV ë¸”ë¡ ì¬ì‚¬ìš© ì„¤ê³„ë¥¼ ë¬¸ì„œë¡œ ê³µê°œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ([docs.vllm.ai](https://docs.vllm.ai/en/v0.10.1.1/design/prefix_caching.html?utm_source=openai))  
- ë‹¨, ìºì‹±ì´ ì¼œì¡ŒëŠ”ë°ë„ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ê¸°ëŒ€ë³´ë‹¤ ë‚®ê²Œ ë³´ì´ëŠ” ì¼€ì´ìŠ¤ë„ ë³´ê³ ë˜ë¯€ë¡œ(ë²„ì „/ëª¨ë¸/ì–‘ìí™” ì¡°í•©) ê´€ì¸¡ê³¼ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤. ([github.com](https://github.com/vllm-project/vllm/issues/8242?utm_source=openai))

3) **TGIëŠ” ì‹ ê·œ ë„ì…ë³´ë‹¤ â€œê¸°ì¡´ ìš´ì˜ì˜ ì•ˆì •ì  ìœ ì§€â€ ìª½ìœ¼ë¡œ**
- HF ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œ TGIëŠ” maintenance modeì…ë‹ˆë‹¤. ì‹ ê·œ ê¸°ëŠ¥/ì„±ëŠ¥ ê²½ìŸì€ vLLM ìª½ì´ ë” í™œë°œí•  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))  
- ì´ë¯¸ TGIë¡œ `/v1/chat/completions` í˜¸í™˜ì„ ì“°ê³  ìˆë‹¤ë©´, ë§ˆì´ê·¸ë ˆì´ì…˜ í”Œëœì„ ë¯¸ë¦¬ ì¤€ë¹„(ìƒˆ endpoint ë³‘í–‰ ìš´ì˜ â†’ íŠ¸ë˜í”½ ì „í™˜)í•˜ëŠ” ê²ƒì´ ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì…ë‹ˆë‹¤. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))

4) **Ollama ìš´ì˜ ì‹œ: ë°”ì¸ë”©/í™˜ê²½ë³€ìˆ˜/ë²„ì „ ì°¨ì´ë¥¼ ë°˜ë“œì‹œ í…ŒìŠ¤íŠ¸**
- ê³µì‹ FAQëŠ” `OLLAMA_HOST`ë¡œ ì™¸ë¶€ ë…¸ì¶œì„ ì•ˆë‚´í•©ë‹ˆë‹¤. ([docs.ollama.com](https://docs.ollama.com/faq?utm_source=openai))  
- ë°˜ë©´ ë³‘ë ¬ì„± ê´€ë ¨ í™˜ê²½ë³€ìˆ˜(`OLLAMA_NUM_PARALLEL` ë“±)ëŠ” ë²„ì „ë³„ ì´ìŠˆê°€ ë³´ê³ ëœ ì ì´ ìˆì–´, â€œë¬¸ì„œ/ë¦´ë¦¬ìŠ¤ë…¸íŠ¸/ì‹¤ì¸¡â€ 3ì¢… ê²€ì¦ì´ ì•ˆì „í•©ë‹ˆë‹¤. ([github.com](https://github.com/ollama/ollama/issues/5722?utm_source=openai))

---

## ğŸš€ ë§ˆë¬´ë¦¬
- **vLLM**: 2026ë…„ 1ì›” ê¸°ì¤€ â€œí”„ë¡œë•ì…˜ LLM ì„œë¹™â€ì˜ ì‚¬ì‹¤ìƒ í‘œì¤€ì— ê°€ê¹ìŠµë‹ˆë‹¤(continuous batching, PagedAttention, OpenAI-compatible, prefix caching, ë©€í‹° GPU ì˜µì…˜). ([marktechpost.com](https://www.marktechpost.com/2025/11/07/comparing-the-top-6-inference-runtimes-for-llm-serving-in-2025/?utm_source=openai))  
- **TGI**: ê¸°ëŠ¥/ì„±ìˆ™ë„ëŠ” ë†’ì§€ë§Œ maintenance mode ì „í™˜ìœ¼ë¡œ ì‹ ê·œ ë„ì…ì€ ì‹ ì¤‘í•˜ê²Œ. ([huggingface.co](https://huggingface.co/docs/inference-endpoints/engines/tgi?utm_source=openai))  
- **Ollama**: ë¡œì»¬ ê°œë°œ/ì‹¤í—˜/ê°œì¸ìš© ì„œë²„ì— ìµœê³ . ë‹¤ì¤‘ ì‚¬ìš©ì ê³ ë¶€í•˜ ì„œë¹„ìŠ¤ëŠ” íŠœë‹/í•œê³„ ì¸ì§€ê°€ í•„ìš”. ([docs.ollama.com](https://docs.ollama.com/api/introduction?utm_source=openai))  

ë‹¤ìŒ í•™ìŠµìœ¼ë¡œëŠ” (1) vLLMì—ì„œ `-tp` ê¸°ë°˜ ë©€í‹° GPU(ë‹¨ì¼ ë…¸ë“œ) êµ¬ì„±, (2) prefix caching íš¨ê³¼ë¥¼ ì¬í˜„í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬(ë™ì¼ prefix ë°˜ë³µ), (3) ê´€ì¸¡( Prometheus/OpenTelemetry )ì„ ë¶™ì—¬ TTFT/ITLì„ ìƒì‹œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” êµ¬ì„±ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
---
title: "FastAPIë¡œ LLM API ì„œë²„ â€œì§„ì§œ ìŠ¤íŠ¸ë¦¬ë°â€ ë§Œë“¤ê¸° (2026ë…„ 2ì›” ê¸°ì¤€): SSE, Cancel, Backpressureê¹Œì§€ í•œ ë²ˆì—"
date: 2026-02-27 02:43:25 +0900
categories: [Backend, API]
tags: [backend, api, trend, 2026-02]
---

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7990TVG7C7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7990TVG7C7');
</script>

## ë“¤ì–´ê°€ë©°
LLM API ì„œë²„ë¥¼ ì§ì ‘ ìš´ì˜í•´ë³´ë©´ â€œì‘ë‹µì´ ëŠë¦¬ë‹¤â€ëŠ” ì‚¬ìš©ì ë¶ˆë§Œì˜ ëŒ€ë¶€ë¶„ì€ **ì´ ì²˜ë¦¬ì‹œê°„**ë³´ë‹¤ **Time-to-first-token(ì²« í† í° ë„ì°© ì‹œê°„)** ì—ì„œ ë°œìƒí•©ë‹ˆë‹¤. íŠ¹íˆ í”„ë¡¬í”„íŠ¸ê°€ ê¸¸ê±°ë‚˜ tool í˜¸ì¶œ/í›„ì²˜ë¦¬ê°€ ë¶™ìœ¼ë©´ ì²« í™”ë©´ì´ ëœ¨ê¸°ê¹Œì§€ ìˆ˜ ì´ˆê°€ ê±¸ë¦¬ê¸°ë„ í•˜ì£ . ì´ë•Œ **Streaming**ì„ ë¶™ì´ë©´ UXê°€ ê¸‰ê²©íˆ ì¢‹ì•„ì§‘ë‹ˆë‹¤.

2026ë…„ 2ì›” í˜„ì¬ë„ ì›¹/ëª¨ë°”ì¼ì—ì„œ ê°€ì¥ í˜„ì‹¤ì ì¸ ì„ íƒì§€ëŠ” í¬ê²Œ ë‘ ê°€ì§€ì…ë‹ˆë‹¤.

- **SSE(Server-Sent Events)**: HTTP ìœ„ì—ì„œ server â†’ client ë‹¨ë°©í–¥ ìŠ¤íŠ¸ë¦¬ë°. ë¸Œë¼ìš°ì €ëŠ” `EventSource`ë¡œ ì¦‰ì‹œ ì†Œë¹„ ê°€ëŠ¥.
- **WebSocket**: ì–‘ë°©í–¥ì´ í•„ìš”í•  ë•Œ ê°•ë ¥í•˜ì§€ë§Œ, ì¸í”„ë¼/í”„ë¡ì‹œ/ì¸ì¦/ìŠ¤ì¼€ì¼ë§ ë³µì¡ë„ê°€ ì˜¬ë¼ê°.

LLM â€œìƒì„± í…ìŠ¤íŠ¸ë¥¼ í˜ë ¤ë³´ë‚´ëŠ”â€ ì „í˜•ì ì¸ APIì—ëŠ” **SSEê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ ë” ë‹¨ìˆœ**í•˜ë‹¤ëŠ” íë¦„ì´ ê°•í•©ë‹ˆë‹¤. ë˜í•œ OpenAIë„ â€œStreaming API responsesâ€ ê°€ì´ë“œë¥¼ SSE ì´ë²¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. ([platform.openai.com](https://platform.openai.com/docs/guides/streaming-responses?utm_source=openai))

---

## ğŸ”§ í•µì‹¬ ê°œë…
### 1) StreamingResponse vs SSE(EventSourceResponse)
- `StreamingResponse`(FastAPI/Starlette): **ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼**ì„ chunked transferë¡œ í˜ë ¤ë³´ë‚´ëŠ” ê°€ì¥ ì €ìˆ˜ì¤€ ë°©ì‹. íŒŒì¼/ë°”ì´ë„ˆë¦¬/í…ìŠ¤íŠ¸ ëª¨ë‘ ê°€ëŠ¥. ([medium.com](https://medium.com/%40ab.hassanein/streaming-responses-in-fastapi-d6a3397a4b7b?utm_source=openai))  
- SSE(`text/event-stream`): ìŠ¤íŠ¸ë¦¬ë° â€œí˜•ì‹(protocol)â€ì´ ì–¹íŒ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼. ë©”ì‹œì§€ í”„ë ˆì´ë°ì´ ëª…í™•í•´ì„œ **í”„ë¡ íŠ¸ì—ì„œ í† í° ë‹¨ìœ„ UI ì—…ë°ì´íŠ¸**ê°€ ì‰½ìŠµë‹ˆë‹¤. ì´ë²¤íŠ¸ëŠ” ì•„ë˜ì²˜ëŸ¼ `\n\n`ë¡œ ì´ë²¤íŠ¸ ê²½ê³„ë¥¼ ìë¦…ë‹ˆë‹¤. ([medium.com](https://medium.com/%40inandelibas/real-time-notifications-in-python-using-sse-with-fastapi-1c8c54746eb7?utm_source=openai))  

SSE wire format(í•µì‹¬ë§Œ):
- í•œ ì´ë²¤íŠ¸ëŠ” ì—¬ëŸ¬ ì¤„ë¡œ êµ¬ì„± ê°€ëŠ¥
- `data: ...` ì—¬ëŸ¬ ì¤„ ê°€ëŠ¥
- ì´ë²¤íŠ¸ ëì€ **blank line(`\n\n`)**
- `:`ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ì€ comment(heartbeatë¡œ í™œìš©)

### 2) LLM Provider ìŠ¤íŠ¸ë¦¬ë° â†’ ìš°ë¦¬ ì„œë²„ SSE ì¬ìŠ¤íŠ¸ë¦¬ë°
ìš”ì¦˜ LLM ProviderëŠ” ëŒ€ì²´ë¡œ â€œstream=Trueâ€ ê°™ì€ ì˜µì…˜ìœ¼ë¡œ **ì´ë²¤íŠ¸/ë¸íƒ€ ìŠ¤íŠ¸ë¦¼**ì„ ì¤ë‹ˆë‹¤. OpenAI Responses APIëŠ” ìŠ¤íŠ¸ë¦¬ë°ì„ â€œtyped semantic eventsâ€ë¡œ ë‚´ë³´ë‚´ë©°, ì˜ˆë¥¼ ë“¤ì–´ `response.output_text.delta` ê°™ì€ ì´ë²¤íŠ¸ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤. ([platform.openai.com](https://platform.openai.com/docs/guides/streaming-responses?utm_source=openai))  
ì„œë²„ëŠ” ì´ë¥¼ ë°›ì•„:
1) ë¸íƒ€ë¥¼ íŒŒì‹±í•˜ê³ 
2) SSE `data:`ë¡œ ê°ì‹¸ì„œ
3) í´ë¼ì´ì–¸íŠ¸ë¡œ ì¦‰ì‹œ flush

### 3) ì‹¤ë¬´ì—ì„œ ì œì¼ ì¤‘ìš”í•œ 3ê°€ì§€: disconnect, heartbeat, backpressure
- **Client disconnect ì²˜ë¦¬**: ë¸Œë¼ìš°ì € íƒ­ ë‹«ì•˜ëŠ”ë° ì„œë²„ê°€ ê³„ì† LLMì„ ë¬¼ê³  ìˆìœ¼ë©´ ë¹„ìš©/ìŠ¤ë ˆë“œ/ì»¤ë„¥ì…˜ì´ ìƒˆëŠ” êµ¬ì¡°ê°€ ë©ë‹ˆë‹¤. `request.is_disconnected()` ì²´í¬ë‚˜, CancelledError/CancelScope ê¸°ë°˜ ì·¨ì†Œê°€ í•µì‹¬ ì´ìŠˆë¡œ ìì£¼ ì–¸ê¸‰ë©ë‹ˆë‹¤. ([github.com](https://github.com/sysid/sse-starlette?utm_source=openai))  
- **Heartbeat(ping)**: ì¤‘ê°„ í”„ë¡ì‹œ/ë¡œë“œë°¸ëŸ°ì„œê°€ â€œì¡°ìš©í•œ ì—°ê²°â€ì„ ëŠì–´ë²„ë¦¬ëŠ” ì¼ì´ ìˆì–´ ì£¼ê¸°ì ìœ¼ë¡œ `: ping\n\n` ê°™ì€ commentë¥¼ í˜ë ¤ ì—°ê²°ì„ ì‚´ë¦½ë‹ˆë‹¤. `sse-starlette`ëŠ” `ping` ì˜µì…˜ë„ ì œê³µí•©ë‹ˆë‹¤. ([github.com](https://github.com/sysid/sse-starlette?utm_source=openai))  
- **Backpressure**: ìƒì‚°ì(LLM ì´ë²¤íŠ¸)ê°€ ë¹ ë¥´ê³  ì†Œë¹„ì(í´ë¼ì´ì–¸íŠ¸ ë„¤íŠ¸ì›Œí¬)ê°€ ëŠë¦´ ë•Œ ë©”ëª¨ë¦¬ ë²„í¼ê°€ ì»¤ì§‘ë‹ˆë‹¤. ë‹¨ìˆœ generatorë³´ë‹¤ **anyio memory channel** íŒ¨í„´ì´ ì•ˆì „í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ([github.com](https://github.com/sysid/sse-starlette?utm_source=openai))  

---

## ğŸ’» ì‹¤ì „ ì½”ë“œ
ì•„ë˜ ì½”ë“œëŠ” **OpenAI ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ ë°›ì•„ SSEë¡œ ì¬ì „ì†¡**í•˜ëŠ” FastAPI ì—”ë“œí¬ì¸íŠ¸ ì˜ˆì œì…ë‹ˆë‹¤. í¬ì¸íŠ¸ëŠ”:
- SSE í”„ë ˆì´ë°ì„ ì§ì ‘ ì‘ì„± (`data: ...\n\n`)
- disconnect ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
- heartbeatë¡œ idle timeout ë°©ì§€
- â€œdoneâ€ ì´ë²¤íŠ¸ë¡œ ì¢…ë£Œ ì‹ í˜¸ ì œê³µ

```python
import asyncio
import json
import time
from typing import AsyncIterator, Optional

from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse

# OpenAI SDK (Responses API streaming)
from openai import OpenAI

app = FastAPI()
client = OpenAI()

def sse_event(data: dict, event: Optional[str] = None) -> bytes:
    """
    SSE í¬ë§·:
      event: <name>\n
      data: <json>\n
      \n
    """
    lines = []
    if event:
        lines.append(f"event: {event}")
    lines.append("data: " + json.dumps(data, ensure_ascii=False))
    return ("\n".join(lines) + "\n\n").encode("utf-8")

async def stream_openai_to_sse(request: Request, prompt: str) -> AsyncIterator[bytes]:
    """
    OpenAI ìŠ¤íŠ¸ë¦¼(typed events)ì„ ë°›ì•„ì„œ SSEë¡œ ë³€í™˜í•´ í˜ë ¤ë³´ë‚¸ë‹¤.
    - disconnect ì‹œ ì¤‘ë‹¨
    - heartbeat ì£¼ê¸°ì ìœ¼ë¡œ ì „ì†¡
    """
    heartbeat_interval = 10.0
    last_sent = time.monotonic()

    # 1) í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ìŠ¤íŠ¸ë¦¼ ì‹œì‘ ì•Œë¦¼(ì„ íƒ)
    yield sse_event({"status": "started"}, event="meta")

    try:
        stream = client.responses.create(
            model="gpt-5",
            input=[{"role": "user", "content": prompt}],
            stream=True,
        )

        for ev in stream:
            # 2) í´ë¼ì´ì–¸íŠ¸ disconnect í™•ì¸ (ë¹„ìš©/ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ ë°©ì§€)
            if await request.is_disconnected():
                break

            now = time.monotonic()
            if now - last_sent >= heartbeat_interval:
                # SSE comment(ì½œë¡  ë¼ì¸)ëŠ” ë¸Œë¼ìš°ì €ê°€ ë¬´ì‹œí•˜ì§€ë§Œ ì—°ê²° ìœ ì§€ì— ë„ì›€
                yield b": ping\n\n"
                last_sent = now

            # 3) OpenAI typed event ì²˜ë¦¬: í…ìŠ¤íŠ¸ deltaë§Œ ì¶”ì¶œí•´ ì „ì†¡
            # OpenAI ê°€ì´ë“œëŠ” typeìœ¼ë¡œ ì´ë²¤íŠ¸ êµ¬ë¶„ì„ ê¶Œì¥ ([platform.openai.com](https://platform.openai.com/docs/guides/streaming-responses?utm_source=openai))
            ev_type = getattr(ev, "type", None) or (ev.get("type") if isinstance(ev, dict) else None)

            if ev_type == "response.output_text.delta":
                delta = getattr(ev, "delta", None) or (ev.get("delta") if isinstance(ev, dict) else "")
                if delta:
                    yield sse_event({"delta": delta}, event="token")
                    last_sent = time.monotonic()

            elif ev_type in ("response.completed", "response.failed", "error"):
                # ì¢…ë£Œ/ì—ëŸ¬ ê³„ì—´ì€ í´ë¼ì´ì–¸íŠ¸ê°€ ì²˜ë¦¬í•˜ê¸° ì‰½ê²Œ ì´ë²¤íŠ¸ë¡œ ì „ë‹¬
                payload = ev.model_dump() if hasattr(ev, "model_dump") else (ev if isinstance(ev, dict) else {"type": ev_type})
                yield sse_event(payload, event="done" if ev_type == "response.completed" else "error")
                break

            # ê·¸ ì™¸ ì´ë²¤íŠ¸ëŠ” í•„ìš” ì‹œ ë¡œê¹…/ë©”íŠ¸ë¦­ìœ¼ë¡œë§Œ ì‚¬ìš©
            # (tool call delta, annotations ë“±)

        # 4) ì •ìƒ ì¢…ë£Œ ì‹ í˜¸(Provider ì¢…ë£Œ ì´ë²¤íŠ¸ë¥¼ ëª» ë°›ëŠ” ê²½ìš° ëŒ€ë¹„)
        yield sse_event({"status": "closed"}, event="meta")

    except asyncio.CancelledError:
        # ì„œë²„ ìª½ ì·¨ì†Œ(í´ë¼ì´ì–¸íŠ¸ ëŠê¹€/ì„œë²„ shutdown) ì‹œ ì •ë¦¬ í¬ì¸íŠ¸
        raise
    except Exception as e:
        yield sse_event({"message": str(e)}, event="error")

@app.get("/v1/chat/stream")
async def chat_stream(request: Request, prompt: str):
    # SSEëŠ” ë°˜ë“œì‹œ text/event-stream
    headers = {
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        # reverse proxyë¥¼ ì“´ë‹¤ë©´ X-Accel-Buffering: no(Nginx) ê°™ì€ ì„¤ì •ë„ ê³ ë ¤
    }
    return StreamingResponse(
        stream_openai_to_sse(request, prompt),
        media_type="text/event-stream",
        headers=headers,
    )
```

---

## âš¡ ì‹¤ì „ íŒ
1) **SSEëŠ” â€œí”„ë¡ì‹œ ë²„í¼ë§â€ì´ ê°€ì¥ í”í•œ í•¨ì •**
- ë¡œì»¬ì—ì„œëŠ” ì˜ ë˜ëŠ”ë° ìš´ì˜(Nginx/Ingress/CDN)ì—ì„œ í† í°ì´ í•œ ë²ˆì— ëª°ì•„ì„œ ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
- `text/event-stream`ì„ ì“°ê³ , í”„ë¡ì‹œì˜ response bufferingì„ ë„ëŠ” ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”(íŠ¹íˆ Nginx/Ingress). â€œMIME typeì´ ì¤‘ìš”í•˜ë‹¤, bufferingë˜ë©´ ì‹¤ì‹œê°„ì„±ì´ ê¹¨ì§„ë‹¤â€ëŠ” ê°€ì´ë“œê°€ ë°˜ë³µë©ë‹ˆë‹¤. ([modal.com](https://modal.com/docs/guide/streaming-endpoints?utm_source=openai))  

2) **disconnect ê°ì§€ëŠ” â€˜í•­ìƒâ€™ ë¯¿ì„ ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•˜ì§€ ë§ê¸°**
- `request.is_disconnected()`ë¡œ ì¶©ë¶„í•œ ê²½ìš°ê°€ ë§ì§€ë§Œ, í™˜ê²½/ë²„ì „/ì„œë²„ ì¡°í•©ì— ë”°ë¼ ì´ìŠˆê°€ ë³´ê³ ë˜ê¸°ë„ í•©ë‹ˆë‹¤. ([gitlab.com](https://gitlab.com/gtucker.io/renelick/-/issues/58?utm_source=openai))  
- ë” ê°•í•˜ê²Œ ê°€ì ¸ê°€ë ¤ë©´ â€œdisconnect watcher taskê°€ CancelScopeë¥¼ ì·¨ì†Œâ€í•˜ëŠ” íŒ¨í„´(ìš”ì§€ëŠ” `request.receive()`ì—ì„œ `http.disconnect`ë¥¼ ê°ì§€)ì„ ê³ ë ¤í•˜ì„¸ìš”. ([fastapiexpert.com](https://fastapiexpert.com/blog/2024/06/06/understanding-client-disconnection-in-fastapi/?utm_source=openai))  

3) **Heartbeat(ping) ì—†ìœ¼ë©´ ìš´ì˜ì—ì„œ ëŠê¸´ë‹¤**
- LLMì´ ì ê¹ ìƒê°(?)í•˜ëŠ” êµ¬ê°„ì— í”„ë¡ì‹œê°€ idle timeoutìœ¼ë¡œ ëŠì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `: ping\n\n` ê°™ì€ comment heartbeatëŠ” êµ¬í˜„ ë‚œì´ë„ ëŒ€ë¹„ íš¨ê³¼ê°€ í½ë‹ˆë‹¤. ([medium.com](https://medium.com/%402nick2patel2/fastapi-server-sent-events-for-llm-streaming-smooth-tokens-low-latency-1b211c94cff5?utm_source=openai))  

4) **Backpressureê°€ ê±±ì •ë˜ë©´ generator ëŒ€ì‹  channel**
- ê°„ë‹¨í•œ ìŠ¤íŠ¸ë¦¼ì€ async generatorë¡œ ì¶©ë¶„í•˜ì§€ë§Œ,
- ìƒì‚°/ì†Œë¹„ ì†ë„ ì°¨ê°€ ì»¤ì§€ë©´ `anyio.create_memory_object_stream()`ë¡œ bounded bufferë¥¼ ë‘ê³  producer/consumerë¥¼ ë¶„ë¦¬í•˜ëŠ” ë°©ì‹ì´ ì•ˆì •ì ì…ë‹ˆë‹¤(íŠ¹íˆ ì—¬ëŸ¬ upstreamì„ í•©ì¹˜ê±°ë‚˜ fan-out í•  ë•Œ). ([github.com](https://github.com/sysid/sse-starlette?utm_source=openai))  

5) **SDK/í´ë¼ì´ì–¸íŠ¸ ì½”ë“œê¹Œì§€ â€œìŠ¤íŠ¸ë¦¬ë° íƒ€ì…â€ì„ ê³ ì •í•˜ë¼**
- ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€ì— ë”°ë¼ ë°˜í™˜ íƒ€ì…ì´ ê°ˆë¦¬ëŠ” ê²½ìš°ê°€ ë§ì•„, íƒ€ì… ì•ˆì •ì„±ì„ ê°œì„ í•˜ë ¤ëŠ” ë„êµ¬/SDKë„ ê³„ì† ë‚˜ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. API ìŠ¤í™ì— `stream: true`ì¼ ë•Œì˜ ì‘ë‹µ ì´ë²¤íŠ¸ë¥¼ ëª…í™•íˆ ë¬¸ì„œí™”í•˜ëŠ” ê²Œ ì¥ê¸°ì ìœ¼ë¡œ ì´ë“ì…ë‹ˆë‹¤. ([speakeasy.com](https://www.speakeasy.com/blog/release-sse-improvements?utm_source=openai))  

---

## ğŸš€ ë§ˆë¬´ë¦¬
FastAPIë¡œ LLM API ì„œë²„ë¥¼ ë§Œë“¤ ë•Œ ìŠ¤íŠ¸ë¦¬ë°ì˜ ë³¸ì§ˆì€ â€œê·¸ëƒ¥ yieldâ€ê°€ ì•„ë‹ˆë¼, **(1) SSE í”„ë ˆì´ë° (2) disconnect ì·¨ì†Œ (3) heartbeat (4) backpressure**ë¥¼ í•¨ê»˜ ì„¤ê³„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. OpenAI ê°™ì€ Providerì˜ typed streaming eventë¥¼ ê·¸ëŒ€ë¡œ í˜ë ¤ë³´ë‚´ì§€ ë§ê³ , **í´ë¼ì´ì–¸íŠ¸ê°€ ì†Œë¹„í•˜ê¸° ì‰¬ìš´ SSE ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ(token/done/error/meta)** ë¡œ ì •ë¦¬í•´ì£¼ë©´ ìš´ì˜ ë‚œì´ë„ê°€ í™• ë–¨ì–´ì§‘ë‹ˆë‹¤. ([platform.openai.com](https://platform.openai.com/docs/guides/streaming-responses?utm_source=openai))  

ë‹¤ìŒ í•™ìŠµìœ¼ë¡œëŠ”:
- `sse-starlette`ì˜ `ping`, `send_timeout`, channel ê¸°ë°˜ streaming êµ¬ì¡° ì´í•´ ([github.com](https://github.com/sysid/sse-starlette?utm_source=openai))  
- tool call/structured outputê¹Œì§€ í¬í•¨í•œ â€œì´ë²¤íŠ¸ ë¼ìš°íŒ…(í…ìŠ¤íŠ¸/íˆ´/ë©”íƒ€)â€ ì„¤ê³„ ([platform.openai.com](https://platform.openai.com/docs/guides/streaming-responses?utm_source=openai))  
ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
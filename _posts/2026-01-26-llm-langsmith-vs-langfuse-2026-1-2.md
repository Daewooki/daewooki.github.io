---
title: "LLM ì•± ìš´ì˜ì˜ í˜„ì‹¤: LangSmith vs Langfuseë¡œ â€œë””ë²„ê¹…Â·ë¹„ìš©Â·í’ˆì§ˆâ€ì„ í•œ ë²ˆì— ì¡ëŠ” ë²• (2026ë…„ 1ì›” ê´€ì )"
date: 2026-01-26 02:32:48 +0900
categories: [AI, MLOps]
tags: [ai, mlops, trend, 2026-01]
---

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7990TVG7C7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7990TVG7C7');
</script>

## ë“¤ì–´ê°€ë©°
LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì€ â€œì˜ ë™ì‘í•œë‹¤/ì•ˆ í•œë‹¤â€ë¡œ ëë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê°™ì€ ì…ë ¥ì—ë„ ì¶œë ¥ì´ í”ë“¤ë¦¬ê³ (ë¹„ê²°ì •ì„±), í•œ ë²ˆì˜ ìš”ì²­ì´ ì—¬ëŸ¬ ë‹¨ê³„(Agent planning â†’ tool â†’ retrieval â†’ LLM â†’ rerankâ€¦)ë¡œ ë¶„ê¸°ë˜ë©°, ë¹„ìš©ì€ í˜¸ì¶œ ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ **ì›Œí¬í”Œë¡œ ì „ì²´**ì—ì„œ ìƒˆì–´ ë‚˜ê°‘ë‹ˆë‹¤. ê·¸ë˜ì„œ ìš´ì˜ ë‹¨ê³„ì—ì„œ ì§„ì§œ í•„ìš”í•œ ê±´ ë‹¨ìˆœ loggingì´ ì•„ë‹ˆë¼ **Observability(ê´€ì¸¡ ê°€ëŠ¥ì„±)** ì…ë‹ˆë‹¤: â€œì–´ë–¤ í”„ë¡¬í”„íŠ¸ê°€â€, â€œì–´ë–¤ ì»¨í…ìŠ¤íŠ¸ë¡œâ€, â€œì–´ë–¤ ëª¨ë¸ì„â€, â€œì–¼ë§ˆë‚˜ ì“°ê³ â€, â€œì–´ë””ì„œ ì‹¤íŒ¨í–ˆëŠ”ì§€â€ë¥¼ **ìš”ì²­ ë‹¨ìœ„(trace)** ë¡œ ì¬êµ¬ì„±í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

2025~2026 íë¦„ì—ì„œ ê°€ì¥ í° ë³€í™”ëŠ” ë‘ ì œí’ˆ ëª¨ë‘ **OpenTelemetry(OTel) ê¸°ë°˜ìœ¼ë¡œ â€˜ì—”ë“œ-íˆ¬-ì—”ë“œ ë¶„ì‚° íŠ¸ë ˆì´ì‹±â€™** ì„ ë°€ê³  ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. LangSmithëŠ” SDK ìˆ˜ì¤€ì˜ OTel ì§€ì›ì„ â€œì™„ì„±ëœ íŒŒì´í”„ë¼ì¸â€ìœ¼ë¡œ í™•ì¥í–ˆê³ (ê¸°ì¡´ì—” ingestion í¬ë§· ì¤‘ì‹¬ì´ì—ˆë‹¤ëŠ” ë§¥ë½), LangfuseëŠ” OTLP ì—”ë“œí¬ì¸íŠ¸ + OTEL-native SDK(v3)ë¡œ ì–¸ì–´/í”„ë ˆì„ì›Œí¬ í˜¸í™˜ì„±ì„ ë„“í˜”ìŠµë‹ˆë‹¤. ([blog.langchain.com](https://blog.langchain.com/end-to-end-opentelemetry-langsmith?utm_source=openai))

---

## ğŸ”§ í•µì‹¬ ê°œë…
### 1) Trace / Span / Observation: â€œí•œ ìš”ì²­ì„ ì¬ì¡°ë¦½í•˜ëŠ” ë‹¨ìœ„â€
- **Trace**: ì‚¬ìš©ì ìš”ì²­ 1ë²ˆì˜ end-to-end ì‹¤í–‰ ë‹¨ìœ„. LLM í˜¸ì¶œ, tool, retrieval, rerank ë“± ì—¬ëŸ¬ ì´ë²¤íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. LangSmithë„ traceë¥¼ â€œë‹¨ì¼ ì‹¤í–‰â€ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤. ([langchain.com](https://www.langchain.com/pricing?utm_source=openai))
- **Span**(OTel): ë¶„ì‚° íŠ¸ë ˆì´ì‹±ì˜ ê¸°ë³¸ ë‹¨ìœ„. ë¶€ëª¨-ìì‹ ê´€ê³„ë¡œ ì¤‘ì²©ë˜ì–´ í˜¸ì¶œ íŠ¸ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
- **Observation( Langfuse )**: LangfuseëŠ” ìˆ˜ì‹ í•œ OTel spanì„ ìì²´ ëª¨ë¸(ìŠ¤íŒ¬/ì œë„ˆë ˆì´ì…˜/ì´ë²¤íŠ¸ ë“±)ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤. GenAI semantic conventionì´ ì§„í™” ì¤‘ì´ë¼ â€œì†ì„± ë§¤í•‘(property mapping)â€ì„ ì œê³µí•œë‹¤ëŠ” ì ì´ í¬ì¸íŠ¸ì…ë‹ˆë‹¤. ([langfuse.com](https://langfuse.com/docs/opentelemetry/get-started?utm_source=openai))

ê²°ë¡ : LLM Observabilityì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ëŠ¥ë ¥ì€ **ì»¨í…ìŠ¤íŠ¸ ì „íŒŒ(Context Propagation)** ì…ë‹ˆë‹¤. ëª¨ë“ˆì´ ê°ˆë¼ì§€ê³  ë¹„ë™ê¸°/ë©€í‹°ìŠ¤ë ˆë“œê°€ ì„ì—¬ë„ â€œì´ tool í˜¸ì¶œì´ ì–´ëŠ ì‚¬ìš©ì ìš”ì²­ì— ì†í•˜ëŠ”ì§€â€ê°€ ìë™ìœ¼ë¡œ ì´ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤. Langfuse SDK v3ê°€ OTel ê¸°ë°˜ìœ¼ë¡œ â€œí‘œì¤€í™”ëœ ì»¨í…ìŠ¤íŠ¸ ì „íŒŒâ€ë¥¼ ì „ë©´ì— ë‚´ì„¸ìš´ ì´ìœ ë„ ì—¬ê¸°ì— ìˆìŠµë‹ˆë‹¤. ([langfuse.com](https://langfuse.com/changelog/2025-05-23-otel-based-python-sdk?utm_source=openai))

### 2) ë””ë²„ê¹…ì˜ ë³¸ì§ˆ: â€œí”„ë¡¬í”„íŠ¸/ì»¨í…ìŠ¤íŠ¸/ì¶œë ¥â€ + â€œì¤‘ê°„ ë‹¨ê³„â€
LLM ì•± ì¥ì• ëŠ” ë³´í†µ ì•„ë˜ ì¤‘ í•˜ë‚˜ë¡œ ê·€ê²°ë©ë‹ˆë‹¤.
- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿/ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë³€ê²½ìœ¼ë¡œ ì„±ëŠ¥ ë¶•ê´´
- retrieval í’ˆì§ˆ ì €í•˜(ì¸ë±ìŠ¤, í•„í„°, top-k, rerank)
- tool I/O ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜(ëª¨ë¸ì´ ë§Œë“  JSONì´ ê¹¨ì§)
- latency ë³‘ëª©(íŠ¹ì • ì™¸ë¶€ API, íŠ¹ì • ëª¨ë¸)
ë”°ë¼ì„œ íŠ¸ë ˆì´ìŠ¤ëŠ” **LLM í˜¸ì¶œë§Œ** ì°ìœ¼ë©´ ë¶€ì¡±í•˜ê³ , **íˆ´/ë¦¬íŠ¸ë¦¬ë²Œ/ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ìŠ¤íŒ¬ê¹Œì§€ ê°™ì€ íŠ¸ë¦¬**ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì´ ì§€ì ì—ì„œ OTelì´ â€œë²¤ë” ì¤‘ë¦½ í‘œì¤€â€ìœ¼ë¡œ í˜ì„ ì–»ê³ , Langfuseê°€ OTLP ì—”ë“œí¬ì¸íŠ¸ë¡œ ë‹¤ì–‘í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ìˆ˜ìš©í•˜ëŠ” ë°©í–¥ì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. ([langfuse.com](https://langfuse.com/changelog/2025-02-14-opentelemetry-tracing?utm_source=openai))

### 3) ë¹„ìš© ì¶”ì ì˜ ë‚œì : â€œí† í° ë¹„ìš© + ë¹„í† í° ë¹„ìš©(íˆ´/ê²€ìƒ‰/ì™¸ë¶€ API)â€
2026ë…„ ìš´ì˜ ê´€ì ì—ì„œ ë¹„ìš©ì€ â€œLLM í† í°â€ë§Œì´ ì•„ë‹™ë‹ˆë‹¤.
- LLM: input/output token, cache read, reasoning token, multimodal token ë“± ì„¸ë¶„í™”
- Tool: ì™¸ë¶€ API ê³¼ê¸ˆ, ë²¡í„°DB ì¿¼ë¦¬ ë¹„ìš©, í¬ë¡¤ë§ ë¹„ìš©
LangSmithëŠ” 2025ë…„ 12ì›”ì— â€œfull-stack cost trackingâ€ì„ ê°•ì¡°í–ˆê³ , UI ê³³ê³³(trace tree / stats / dashboards)ì—ì„œ í† í°Â·ë¹„ìš© breakdownì„ ë³´ì—¬ì£¼ëŠ” êµ¬ì¡°ë¥¼ ê°–ì·„ìŠµë‹ˆë‹¤. ([changelog.langchain.com](https://changelog.langchain.com/announcements/unified-cost-tracking-for-llms-tools-retrieval?utm_source=openai))  
ë˜í•œ LangSmithëŠ” ìë™ ê³„ì‚°(í† í°+ê°€ê²©í‘œ) + ìˆ˜ë™ ì œì¶œ(ì»¤ìŠ¤í…€ ë¹„ìš©)ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤. ([docs.langchain.com](https://docs.langchain.com/langsmith/cost-tracking?utm_source=openai))

Langfuseë„ OTEL-native SDK v3ì—ì„œ **token usage, cost tracking, scoring** ê°™ì€ LLM íŠ¹í™” í—¬í¼ë¥¼ â€œOTel ìœ„ thin layerâ€ë¡œ ì œê³µí•œë‹¤ê³  ëª…ì‹œí•©ë‹ˆë‹¤. ([langfuse.com](https://langfuse.com/docs/opentelemetry/get-started?utm_source=openai))

---

## ğŸ’» ì‹¤ì „ ì½”ë“œ
ì•„ë˜ ì˜ˆì‹œëŠ” â€œìš°ë¦¬ ì•± ì½”ë“œëŠ” OTelë¡œë§Œ ê³„ì¸¡í•˜ê³ , ë°±ì—”ë“œëŠ” Langfuse ë˜ëŠ” LangSmithë¡œ ë°”ê¿” ë¼ìš¸ ìˆ˜ ìˆê²Œâ€ ë§Œë“œëŠ” ì ‘ê·¼ì…ë‹ˆë‹¤. í•µì‹¬ì€ **OTLP exporter ì„¤ì • + span attributeì— GenAI/ë¹„ìš© ì •ë³´ë¥¼ ì‹¤ì–´ ë³´ë‚´ê¸°** ì…ë‹ˆë‹¤.

```python
# Python 3.11+
# pip install opentelemetry-sdk opentelemetry-exporter-otlp openai
#
# ëª©ì :
# 1) OTelë¡œ trace/span ìƒì„±
# 2) LLM í˜¸ì¶œ/íˆ´ í˜¸ì¶œì„ ê°™ì€ trace íŠ¸ë¦¬ì— ë„£ìŒ
# 3) token/cost(ê°€ëŠ¥í•˜ë©´)ë¥¼ span attributeë¡œ ë‚¨ê²¨ ìš´ì˜ ë¶„ì„ ê°€ëŠ¥í•˜ê²Œ í•¨
#
# ë°±ì—”ë“œ ì„ íƒ:
# - Langfuse: OTLP endpointë¡œ ingest ê°€ëŠ¥ (/api/public/otel).  ([langfuse.com](https://langfuse.com/integrations/native/opentelemetry?utm_source=openai))
# - LangSmith: OTel ê¸°ë°˜ tracingì„ ì§€ì›(ìì²´ íŒŒì´í”„ë¼ì¸/ê°€ì´ë“œ ì¡´ì¬). ([docs.langchain.com](https://docs.langchain.com/langsmith/trace-with-opentelemetry?utm_source=openai))

import os
import time
from opentelemetry import trace
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter

# ---------- 1) OTLP Exporter ì„¤ì • ----------
# Langfuseë¥¼ ì“´ë‹¤ë©´ ì˜ˆì‹œ:
#   OTEL_EXPORTER_OTLP_ENDPOINT="https://<langfuse-host>/api/public/otel"
#   OTEL_EXPORTER_OTLP_HEADERS="x-langfuse-public-key=... ,x-langfuse-secret-key=..."
#
# ì‹¤ì œ í—¤ë” í‚¤/í˜•ì‹ì€ Langfuse ë¬¸ì„œì— ë§ì¶”ì„¸ìš”. (ì—¬ê¸°ì„  êµ¬ì¡°ë§Œ ì œì‹œ)
# LangSmithì˜ ê²½ìš°ë„ OTel exporter êµ¬ì„±ì´ ê°€ëŠ¥í•˜ë©°, LangChain/LangGraphëŠ” ìë™ ê³„ì¸¡ ì˜µì…˜ë„ ìˆìŠµë‹ˆë‹¤. ([docs.langchain.com](https://docs.langchain.com/langsmith/trace-with-opentelemetry?utm_source=openai))

endpoint = os.environ.get("OTEL_EXPORTER_OTLP_ENDPOINT")
headers = os.environ.get("OTEL_EXPORTER_OTLP_HEADERS", "")

resource = Resource.create({
    "service.name": "llm-app",
    "deployment.environment": os.environ.get("APP_ENV", "dev"),
})

provider = TracerProvider(resource=resource)
trace.set_tracer_provider(provider)

exporter = OTLPSpanExporter(endpoint=endpoint, headers=headers)
provider.add_span_processor(BatchSpanProcessor(exporter))

tracer = trace.get_tracer(__name__)

# ---------- 2) ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: LLM + Tool ----------
def fake_tool_search(query: str) -> dict:
    # íˆ´ í˜¸ì¶œë„ spanìœ¼ë¡œ ê°ì‹¸ë©´, ì–´ëŠ ìš”ì²­ì˜ ì–´ëŠ ë‹¨ê³„ì¸ì§€ í•œëˆˆì— ë³´ì…ë‹ˆë‹¤.
    with tracer.start_as_current_span("tool.search") as span:
        t0 = time.time()
        time.sleep(0.05)
        span.set_attribute("tool.name", "fake_search")
        span.set_attribute("tool.query", query)
        # ë¹„í† í° ë¹„ìš©(ì™¸ë¶€ API ê³¼ê¸ˆ ë“±)ì´ ìˆìœ¼ë©´ attributeë¡œ ë‚¨ê¸°ê±°ë‚˜
        # (LangSmithëŠ” tool runì— total_costë¥¼ ë„£ëŠ” íŒ¨í„´ë„ ì§€ì›) ([docs.langchain.com](https://docs.langchain.com/langsmith/cost-tracking?utm_source=openai))
        span.set_attribute("cost.usd", 0.0003)
        span.set_attribute("latency.ms", int((time.time() - t0) * 1000))
        return {"docs": ["doc1 about " + query, "doc2 about " + query]}

def call_llm(prompt: str) -> str:
    with tracer.start_as_current_span("llm.call") as span:
        t0 = time.time()
        # ì‹¤ì œë¡œëŠ” OpenAI/Anthropic SDK í˜¸ì¶œ
        # ì—¬ê¸°ì„  ì˜ˆì‹œë¡œ ê³ ì • ì‘ë‹µ
        completion = f"Answer based on: {prompt[:40]}..."

        # ìš´ì˜ì— ì¤‘ìš”í•œ ìµœì†Œ ì†ì„±ë“¤
        span.set_attribute("llm.model", os.environ.get("LLM_MODEL", "gpt-*"))
        span.set_attribute("llm.prompt_chars", len(prompt))
        span.set_attribute("latency.ms", int((time.time() - t0) * 1000))

        # í† í°/ë¹„ìš©ì€ ê³µê¸‰ì ì‘ë‹µì—ì„œ usageë¥¼ íŒŒì‹±í•´ ë„£ëŠ” ê²Œ ì •ì„ì…ë‹ˆë‹¤.
        # LangSmithëŠ” usage_metadata ê¸°ë°˜ìœ¼ë¡œ ìë™/ìˆ˜ë™ ë¹„ìš© ê³„ì‚°ì„ ì§€ì›í•©ë‹ˆë‹¤. ([docs.langchain.com](https://docs.langchain.com/langsmith/cost-tracking?utm_source=openai))
        # (OTel-onlyë¡œ ê°„ë‹¤ë©´ attributeì— ë‚¨ê²¨ë„ ë¶„ì„ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.)
        span.set_attribute("tokens.input", 120)
        span.set_attribute("tokens.output", 80)
        span.set_attribute("cost.usd", 0.0021)

        return completion

def handle_request(user_query: str, user_id: str) -> str:
    # Traceì˜ ìµœìƒìœ„ span: request ë‹¨ìœ„
    with tracer.start_as_current_span("request") as span:
        span.set_attribute("user.id", user_id)
        span.set_attribute("request.query", user_query)

        ctx = fake_tool_search(user_query)
        prompt = f"Q: {user_query}\nContext: {ctx['docs']}\nA:"
        answer = call_llm(prompt)

        span.set_attribute("result.len", len(answer))
        return answer

if __name__ == "__main__":
    print(handle_request("LangSmithì™€ Langfuse ì°¨ì´?", "user-123"))
```

ì´ ë°©ì‹ì˜ ì¥ì ì€ â€œê³„ì¸¡ì€ OTel í‘œì¤€ìœ¼ë¡œ ê³ ì •â€í•˜ê³ , ë°±ì—”ë“œëŠ” ì¡°ì§ ìƒí™©ì— ë”°ë¼ Langfuse(ì˜¤í”ˆì†ŒìŠ¤/ì…€í”„í˜¸ìŠ¤íŠ¸) ë˜ëŠ” LangSmith(í‰ê°€/í”Œë«í¼ í†µí•© í¬í•¨)ë¡œ ì„ íƒì§€ë¥¼ ë‚¨ê¸¸ ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë‘ ì œí’ˆ ëª¨ë‘ OTelì„ í•µì‹¬ ì¶•ìœ¼ë¡œ ê°•í™”í•˜ëŠ” ì¶”ì„¸ë¼, ì¥ê¸° ìœ ì§€ë³´ìˆ˜ì—ë„ ìœ ë¦¬í•©ë‹ˆë‹¤. ([blog.langchain.com](https://blog.langchain.com/end-to-end-opentelemetry-langsmith?utm_source=openai))

---

## âš¡ ì‹¤ì „ íŒ
1) **Samplingì„ â€˜í™˜ê²½ë³„â€™ë¡œ ë‹¤ë¥´ê²Œ**
- dev/staging: 100% trace
- prod: ì—ëŸ¬/ì§€ì—°/ê³ ë¹„ìš© ìš”ì²­ì€ 100%, ë‚˜ë¨¸ì§€ëŠ” ìƒ˜í”Œë§  
OTel íŒŒì´í”„ë¼ì¸ì„ ì“°ë©´ í‘œì¤€ì ì¸ ë°©ì‹ìœ¼ë¡œ sampling ì „ëµì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ë²¤ë” ì¢…ì† ìµœì†Œí™”).

2) **ë¹„ìš© ì¶”ì ì€ â€œìë™ + ìˆ˜ë™â€ í˜¼í•©ì´ í˜„ì‹¤ì **
- LLM ë¹„ìš©ì€ í† í° usageê°€ ì˜ ë‚˜ì˜¤ë©´ ìë™ ì§‘ê³„ê°€ í¸í•©ë‹ˆë‹¤(LangSmithëŠ” ëª¨ë¸ ê°€ê²©í‘œ/ì„¸ë¶€ í† í° íƒ€ì…ê¹Œì§€ ë§¤í•‘ ê°€ëŠ¥). ([docs.langchain.com](https://docs.langchain.com/langsmith/cost-tracking?utm_source=openai))
- tool/retrieval/ì™¸ë¶€ APIëŠ” **ìˆ˜ë™ ë¹„ìš© ì œì¶œ** ë˜ëŠ” span attributeë¡œ ë‚¨ê²¨ì„œ â€œì›Œí¬í”Œë¡œ ì´ë¹„ìš©â€ì„ ì™„ì„±í•˜ì„¸ìš”. LangSmithëŠ” tool runì— `total_cost` ê°™ì€ í˜•íƒœë¡œ ë¹„ìš©ì„ ë¶™ì´ëŠ” ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ([docs.langchain.com](https://docs.langchain.com/langsmith/cost-tracking?utm_source=openai))

3) **ì»¨í…ìŠ¤íŠ¸ ì „íŒŒê°€ ê¹¨ì§€ëŠ” ì§€ì ì„ ë¨¼ì € ì˜ì‹¬**
- ë¹„ë™ê¸° ì‘ì—…, ë°±ê·¸ë¼ìš´ë“œ í, ë©€í‹°í”„ë¡œì„¸ìŠ¤ì—ì„œ traceê°€ ëŠì–´ì§€ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
- Langfuse SDK v3ê°€ OTel ê¸°ë°˜ìœ¼ë¡œ â€œìë™ ì»¨í…ìŠ¤íŠ¸ ì „íŒŒ/ì¤‘ì²©(span nesting)â€ë¥¼ ê°•ì¡°í•˜ëŠ” ì´ìœ ê°€ ì—¬ê¸° ìˆìŠµë‹ˆë‹¤. ([langfuse.com](https://langfuse.com/changelog/2025-05-23-otel-based-python-sdk?utm_source=openai))

4) **ë³´ê´€(retention)ê³¼ ê³¼ê¸ˆ ëª¨ë¸ì„ ìš´ì˜ ì •ì±…ì— ë§ì¶°ë¼**
- LangSmithëŠ” trace ë³´ê´€ ê¸°ê°„ì— ë”°ë¼ base/extendedë¡œ êµ¬ë¶„ë˜ë©° ê°€ê²©ì´ ë‹¤ë¦…ë‹ˆë‹¤(14ì¼ vs 400ì¼). ì¥ì•  ë¶„ì„ì´ â€œìµœê·¼ 2ì£¼â€ì¸ì§€ â€œì¥ê¸° í’ˆì§ˆ ì¶”ì â€ì¸ì§€ì— ë”°ë¼ ì„ íƒì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ([langchain.com](https://www.langchain.com/pricing?utm_source=openai))
- ë˜í•œ LangSmithëŠ” â€œë‹¬ëŸ¬ ê¸°ì¤€ spend limitâ€ì´ ì•„ë‹ˆë¼ â€œtrace ìˆ˜ ì œí•œâ€ì„ ì„¤ì •í•˜ëŠ” êµ¬ì¡°ë¼ëŠ” ì ì´ í•¨ì • í¬ì¸íŠ¸ì…ë‹ˆë‹¤(ì˜ˆì‚° í†µì œë¥¼ ìˆ«ì ë³€í™˜í•´ì„œ ìš´ì˜í•´ì•¼ í•¨). ([docs.langchain.com](https://docs.langchain.com/langsmith/pricing-faq?utm_source=openai))

5) **GenAI semantic conventionsëŠ” ì•„ì§ ì§„í™” ì¤‘: attribute ë„¤ì´ë°ì„ ì •í•´ë‘ê¸°**
Langfuseë„ ëª…ì‹œí•˜ë“¯ GenAIìš© OTel ì†ì„± ê·œì•½ì€ ê³„ì† ë³€í•©ë‹ˆë‹¤. ([langfuse.com](https://langfuse.com/docs/opentelemetry/get-started?utm_source=openai))  
íŒ€ ë‚´ë¶€ í‘œì¤€(ì˜ˆ: `llm.model`, `tokens.input`, `cost.usd`, `rag.top_k`, `tool.name`)ì„ ë¨¼ì € ê³ ì •í•´ë‘ë©´, ë°±ì—”ë“œ/ë¼ì´ë¸ŒëŸ¬ë¦¬ ë³€ê²½ì—ë„ ë°ì´í„°ê°€ â€œë¹„êµ ê°€ëŠ¥â€í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.

---

## ğŸš€ ë§ˆë¬´ë¦¬
2026ë…„ 1ì›” ê¸°ì¤€ìœ¼ë¡œ LangSmithì™€ Langfuseì˜ ê³µí†µëœ í° ë°©í–¥ì€ ëª…í™•í•©ë‹ˆë‹¤: **OTelì„ ì¤‘ì‹¬ìœ¼ë¡œ LLM Observabilityë¥¼ â€œë¶„ì‚° ì‹œìŠ¤í…œ ê´€ì¸¡â€ì˜ ì„¸ê³„ë¡œ ëŒì–´ì˜¤ëŠ” ê²ƒ**. LangSmithëŠ” í‰ê°€/ëŒ€ì‹œë³´ë“œ/ë¹„ìš© ì§‘ê³„ë¥¼ ì œí’ˆ ë‚´ì—ì„œ ê°•í•˜ê²Œ í†µí•©í•˜ê³ (íŠ¹íˆ full-stack cost tracking), LangfuseëŠ” OTLP ìˆ˜ì‹  + OTEL-native SDKë¡œ â€œí˜¸í™˜ì„±ê³¼ ì˜¤í”ˆ ìƒíƒœê³„â€ë¥¼ ë„“íˆëŠ” ì „ëµì´ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ([changelog.langchain.com](https://changelog.langchain.com/announcements/unified-cost-tracking-for-llms-tools-retrieval?utm_source=openai))

ë‹¤ìŒ í•™ìŠµ ì¶”ì²œ:
- OTelì—ì„œ **context propagation**(async, background job) ì œëŒ€ë¡œ ì¡ê¸°
- â€œë¹„ìš©â€ì„ LLM í† í°ì— í•œì •í•˜ì§€ ì•Šê³  **tool/retrievalê¹Œì§€ í•©ì‚°**í•˜ëŠ” ë°ì´í„° ëª¨ë¸ ì„¤ê³„
- Prodì—ì„œì˜ **sampling/PII ë§ˆìŠ¤í‚¹/retention ì •ì±…**ê¹Œì§€ í¬í•¨í•œ ìš´ì˜ ì„¤ê³„

ì›í•˜ì‹œë©´, (1) LangSmith ì „ìš© ì½”ë“œ( `langsmith[otel]`, `LANGSMITH_OTEL_ENABLED` ê¸°ë°˜)ë¡œ ì™„ì „íˆ ë§ì¶˜ ì˜ˆì œ, (2) Langfuse SDK v3(`observe`, `get_client`) ìŠ¤íƒ€ì¼ ì˜ˆì œë¡œë„ ê°ê° ë”°ë¡œ ì •ë¦¬í•´ ë“œë¦´ê²Œìš”. ([docs.langchain.com](https://docs.langchain.com/langsmith/trace-with-opentelemetry?utm_source=openai))